\begin{EnAbstract}
% Building a retrieval system with lifelogging data is more complicated than with ordinary data due to the redundancies, blurriness, massive amount of data, various sources of information accompanying lifelogging data, and especially the ad-hoc nature of queries. In this thesis, we propose an improved version of FIRST 3.0, the system that was used in the Lifelog Search Challenge (LSC) 2022. 
% The Lifelog Search Challenge (LSC) is a benchmarking challenge that encourages researchers and developers to push the boundaries in lifelog retrieval. 
% For LSC'22, we develop \textbf{FIRST 3.0}, a novel and flexible system that leverages expressive cross-domain embeddings to enhance the searching process.
% Based on recent advances in joint vision-language understanding, we propose multiple components that together enable fast and effective retrieval.  
% Our system aims to adaptively capture the semantics of an image at different levels of detail. We also propose to augment our system with an external search engine to help our system with initial visual examples for unfamiliar concepts.
% Critically, to enhance the users' belief in the retrieval results, we aim to provide indication on how the results are generated. 
% Specifically, for the first step, we propose to segment the objects being referred to in the text query. 
% Finally, we organize image data in hierarchical clusters based on their visual similarity and location to assist users in data exploration. 
% Our segmentation framework is competitive across several datasets.
% Overall, our user study and experiments show that our system is both fast and effective in handling various retrieval scenarios, and provides helpful indication on the results.

% \begin{spacing}{1.4} 
% Visual data retrieval is crucial task given the ubiquitous amount of visual data that is left unattended. 
% One main application is to retrieve images corresponding to a description, which is helpful in many situations, such as locating abnormal events in surveillance cameras footage or managing a large collection of captured photos. 
% Lifelogging is a perfect example, as the lifelogger wants to digitize their everyday life, and only detailed analysis of the immense data generated will achieve this.

% In this thesis, we propose an improved version of FIRST 3.0, a Flexible Interactive Retrieval SysTem that leverages expressive cross-domain embeddings to enhance the searching process.
% Specifically, we propose to enrich the image representation by incorporating features at different level of details to capture both global and fine-grained information.
% We also introduce a novel feature that allows users to summarize a long sequence of images to a short one by selectively keeping key frames, with adjustable granularity.
% This feature allows users to quickly review events that happened in a time frame (e.g., a day), which is usually impossible due to the disproportionate number of similar looking images that contain little to no information.
% To help users make the most of our system, we conducted a user study and composed the best practices in utilizing our system based on analysis and our experience. 
% Our system achieves rank 7/16 in the Lifelog Search Challenge 2022 and 4/9 in the Visual Browser Showdown 2022.

% Heading towards Explainable Retrieval, we propose the addition of a Referring Expression Segmentation module to provide an intuitive explanation of the results.
% This module is highly compatible with our system as it shares the input, at the same time complementing it by pointing out precisely the object being referred to, consolidating the user's belief on the results.
% Our novel Visual-Linguistic Transformer Block associates visual and linguistic features with the object queries to produce a more fine-grained representation of object queries.
% Our method named VLFormer achieves state-of-the-art results across multiple benchmarks and is currently under review for the AAAI 2023 conference.
% \end{spacing}


%eureka

\begin{spacing}{1.4} 
Visual data retrieval is crucial task given the ubiquitous amount of visual data that is left unattended. 
One main application is to retrieve images corresponding to a description, which is helpful in many situations, such as locating abnormal events in surveillance cameras footage or managing a large collection of captured photos. 
Lifelogging is a perfect example, as the lifelogger wants to digitize their everyday life, and only detailed analysis of the immense data generated will achieve this.

In this project, we propose an improved version of FIRST 3.0, a Flexible Interactive Retrieval SysTem that leverages expressive cross-domain embeddings to enhance the searching process.
Specifically, we propose to enrich the image representation by incorporating features at different level of details to capture both global and fine-grained information.
We also introduce a novel feature that allows users to summarize a long sequence of images to a short one by selectively keeping key frames, with adjustable granularity.
This feature allows users to quickly review events that happened in a time frame (e.g., a day), which is usually impossible due to the disproportionate number of similar looking images that contain little to no information.
To help users make the most of our system, we conducted a user study and composed the best practices in utilizing our system based on analysis and our experience. 
Our system achieves rank 7/16 in the Lifelog Search Challenge 2022 and 4/9 in the Visual Browser Showdown 2022.

Heading towards Explainable Retrieval, we propose the addition of a Referring Expression Segmentation module to provide an intuitive explanation of the results.
This module is highly compatible with our system as it shares the input, at the same time complementing it by pointing out precisely the object being referred to, consolidating the user's belief on the results.
Our novel Visual-Linguistic Transformer Block associates visual and linguistic features with the object queries to produce a more fine-grained representation of object queries.
Our method named VLFormer achieves state-of-the-art results across multiple benchmarks and is currently under review for the AAAI 2023 conference.
\end{spacing}

\end{EnAbstract}