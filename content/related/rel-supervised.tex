\section{Supervised approaches} 
\label{section:rel-supervised}
Supervised methods rely on datasets with human-labeled ground-truth annotations. For example, the SumMe dataset \cite{Gygli2014SumMe} utilizes video summaries as ground truth, while the TVSum dataset \cite{Song2015TVSum} employs frame-level importance scores. By leveraging this labeled data, supervised approaches aim to learn the criteria for selecting video frames or fragments to construct effective video summaries.

\subsection{Supervision on frame importance with inter-frame temporal dependency}
\label{subsec:rel-sup-temporal-dependency}

% A number of deep learning-based supervised methods have been proposed for video summarization, but there are also unsupervised approaches that address the lack of ground-truth data by leveraging adversarial learning techniques. These unsupervised methods aim to create video summaries that are representative of the original video content and can be reconstructed from the summary itself.

% One approach is based on Generative Adversarial Networks (GANs), where a Summarizer architecture consists of a Key-frame Selector that estimates frame importance and generates a summary, and a Generator that reconstructs the original video based on the generated summary. The reconstructed video and the original video are then evaluated by a Discriminator, which tries to distinguish between them. The Summarizer's objective is to fool the Discriminator, resulting in a summary that closely resembles the original video. Mahasseni et al. (2017) introduced an LSTM-based key-frame selector combined with a Variational Auto-Encoder (VAE) and a Discriminator to learn video summarization through adversarial learning. Apostolidis et al. (2019) further improved this approach with a stepwise, label-based training strategy, leading to enhanced summarization performance. Yuan et al. (2019) proposed a method that maximizes the mutual information between the summary and the video using a couple of discriminators and a cycle-consistent adversarial learning objective.

% Other works have extended the VAE-GAN architecture by incorporating attention mechanisms. Jung et al. (2019) introduced a chunk and stride network (CSNet) and a difference attention mechanism to assess frame dependency at different temporal granularities. They later introduced a self-attention mechanism combined with an algorithm for modeling relative positions between frames. Apostolidis et al. (2020) replaced the VAE with a deterministic Attention Auto-Encoder, improving the key-frame selection process. He et al. (2019) proposed a self-attention-based conditional GAN with a conditional feature selector and a multi-head self-attention mechanism to capture long-range temporal dependencies. Rochan et al. (2019) developed an adversarial approach for video summarization from unpaired data, using GANs and a Fully-Convolutional Sequence Network (FCSN) encoder-decoder.

% These deep learning methods provide alternative strategies for video summarization by learning from the data itself, without relying on ground-truth annotations. They aim to generate representative summaries that can be reconstructed into the original video, leveraging adversarial training techniques and attention mechanisms to improve the quality and diversity of the generated summaries.

\subsection{Supervision on summary authenticity with discriminative adversarial learning}
\label{subsec:rel-sup-spatiotemporal}

% In order to improve the estimation of video frame/fragment importance, certain techniques focus on capturing both the spatial and temporal structure of the video. These approaches not only take into account the input sequence of video frames and the available ground-truth data indicating frame importance, but also model the spatiotemporal dependencies among frames. This additional analysis enhances the training process of the Summarizer, as shown by the dashed rectangles and lines in Figure 3.

% Lal et al. (2019) introduced an encoder-decoder architecture with convolutional LSTMs that effectively models the spatiotemporal relationships within the video. The algorithm not only estimates frame importance but also enhances visual diversity through next frame prediction and shot detection mechanisms, leveraging the likelihood that initial frames of a shot are often part of the summary.

% Yuan et al. (2019) employed a trainable 3D-CNN to extract deep and shallow features from the video content and fused them to create a new representation. This representation, combined with convolutional LSTMs, captures the spatial and temporal structure of the video. A novel loss function called Sobolev loss is then used to learn summarization by minimizing the distance between the series of frame-level importance scores and the ground-truth scores, effectively exploiting the temporal structure of the video.

% Chu et al. (2019) leveraged CNNs to extract spatial and temporal information from raw frames and optical flow maps. Through a label distribution learning process, they learned to estimate frame importance based on human annotations. 

% Elfeki et al. (2019) combined CNNs and Gated Recurrent Units (GRUs), a type of RNN, to form spatiotemporal feature vectors. These vectors were used to estimate the level of activity and importance for each frame.

% Huang et al. (2020) trained a neural network to extract spatiotemporal information from the video, specifically focusing on inter-frame motion. This information was used to create an inter-frames motion curve, which was then input into a transition effects detection method for shot segmentation. A self-attention model, guided by human-generated ground-truth data, was employed to estimate intra-shot importance and select key frames/fragments for creating static/dynamic video summaries.

% By incorporating the spatial and temporal aspects of videos, these supervised approaches improve the accuracy of frame importance estimation and enable the generation of more informative video summaries.

\subsection{Supervision on summary authenticity with discriminative adversarial learning}
\label{subsec:rel-sup-discriminative}

% Taking a distinct approach to bridge the gap between machine-generated and ground-truth summaries, certain methods leverage Generative Adversarial Networks (GANs). Illustrated in Figure 4, the Summarizer, acting as the GAN's Generator, takes the video frames as input and generates a summary by computing frame-level importance scores. These predicted scores, along with an optimal video summary based on user preferences, are fed to a trainable Discriminator, which evaluates their similarity and outputs a corresponding score. The training process encompasses an adversarial framework where the Summarizer aims to deceive the Discriminator by producing summaries that are indistinguishable from the user-generated ones, while the Discriminator learns to differentiate between them. When the Discriminator's confidence level becomes low, indicating an equal classification error for both machine- and user-generated summaries, the Summarizer successfully generates summaries that align closely with users' expectations. Zhang et al. (2019) introduced a method that employs LSTMs and Dilated Temporal Relational (DTR) units to capture temporal dependencies across different time windows. Their approach trains the Summarizer by attempting to mislead a trainable discriminator into distinguishing between machine-generated summaries, ground-truth summaries, and randomly created summaries. Fu et al. (2019) proposed an adversarial learning technique for (semi-)supervised video summarization, where the Generator/Summarizer is an attention-based Pointer Network. It determines the start and end points of each video fragment used in the summary. The Discriminator, a 3D-CNN classifier, determines whether a fragment belongs to a ground-truth or machine-generated summary. Instead of using a conventional adversarial loss, their algorithm employs the output of the Discriminator as a reward for training the Generator/Summarizer through reinforcement learning. While the use of GANs in supervised video summarization is relatively limited, this machine learning framework has been extensively employed in unsupervised video summarization, which will be discussed in the subsequent section.
