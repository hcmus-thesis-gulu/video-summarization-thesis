\section{An extension of Referring Video Object Segmentation}
\label{sec:rvos_inference}
Given a video clip $V = \{V_1, V_2, ..., V_T\}$ and a referring expression with $L$ words. Our approach is to generate the set of $N$ predictions for each frame in $T$ frames. And the $i$-th prediction in all $T$ frames is to track and segment the same object. We can maintain the same relative positions due to our efficient architecture design of sharing object queries between frames.

% \subsection{Instance Sequence Matching and Loss}
% Our approach is to generate the set of $N$ predictions for each frame in $T$ frames. And the $i$-th prediction in all $T$ frames is to track and segment the same object. We can maintain the same relative positions due to sharing object queries between frames.

% Let us denote the prediction set as $\hat{y} = \{\hat{y_i}_{i = 1}^{N}\}$, and the prediction for the $i$-th instance is represented by:
% \begin{equation}
%     \hat{y}_i = \{\hat{p}_i^t, \hat{s}_i^t\}_{i = 1}^T
% \end{equation}

% For the $t$-th frame, $\hat{p}_i^t \in \mathbb{R}^1$ is a probability that this instance corresponds to the referred object or not. And $\hat{s}_i^t \in \mathbb{R}^{H \times W}$ is the segmentation mask that we predict.

% Since there is only one referred object in the video, the ground-truth instance sequence is represented as $y = \{s^t\}_{t = 1}^T$. To train the network, we first find the best prediction $i$-th from $N$ candidates via minimizing the matching cost:

% % % \usepackage{amsmath}
% % % \DeclareMathOperator*{\argmax}{arg\,max}
% % % \DeclareMathOperator*{\argmin}{arg\,min}

% \begin{equation}
%     \mathcal{L}_{match}(y, \hat{y}_i) = \gamma_{cls}\mathcal{L}_{cls}(\hat{p}_i^t, 1) + \gamma_{mask}\mathcal{L}_{mask}(s_i^t, \hat{s}_i^t) + \gamma_{dice}\mathcal{L}_{dice}(s_i^t, \hat{s}_i^t)
% \end{equation}

% % $\mathcal{L}_{cls}$ represents the loss function for the probability a query is the referred object and we use the Cross Entropy loss in this work. $\mathcal{L}_{mask}$ is the binary focal loss that is designed to supervise the mask prediction. And the $\mathcal{L}_{dice}$ is added to improve the dice score, which is quite similar to the IoU metric.
% % $\gamma_{cls}, \gamma_{mask}$ and $\gamma_{dice}$ are the coefficients of corresponding losses. 


% Our goal is to minimize the $\mathcal{L}_{macth}$ of one query and the probabilities of other queries $\hat{p}_j (j \not= i) $ approximate zero (it means these queries do not represent for the referred object). Therefore, our loss function is described as follows: 
% \begin{equation}
%     \mathcal{L}(y, \hat{y}, i) = \mathcal{L}_{match}(y, \hat{y}_i) + \sum_{\substack{j = 1 \\ j \not=i}}^{N}{\gamma_{cls}\mathcal{L}_{cls}(\hat{p}_j^t, 0)}
% \end{equation}

\subsection{Selection strategy}

After performing a multiplication between the final object queries and the pixel-level embeddings from the pixel decoder, we obtain the probability segmentation map for $N$ candidate instances $S \in \mathbb{R}^{N \times T \times H \times W}$
Among $N$ candidates, we need a strategy to choose the correct one. Our strategy is to use the predicted probabilities to generate the confidence score for each candidate in the whole video. We obtain the confidence score set $P_i = \{p_{i}\}^N_{j = 1}$ by averaging the predicted probabilities over all the frames for each instance query. We select the instance with the highest score, and its index is donated as $\sigma$. 

The segmentation mask for each frame is obtained from the mask candidates set by selecting the corresponding query index $\sigma$. 

\subsection{Post-processing}
The prediction is not always segmented smoothly and continuously in case there are several objects that are similar to the referred object. To deal with this, semi-supervised Video Object Segmentation(Semi-VOS) methods can handle the problem. Semi-VOS aims to segment objects given the ground truth of them for the first frame.   

Regarding our problem, there is no ground-truth mask for any frames in the inference. However, after predicting the whole video, we have already had the preliminary results, and we can leverage these predictions to refine the target masks by using a Semi-VOS method. STCN is the state-of-the-art in the Semi-VOS tasks by constructing a memory bank of several pair frames and their annotations and utilizing the information from that bank to segment the corresponding objects in the query frame. 

We do not have any actual ground truth, which means we can not identify which object in which frame is the correct one to reference. We assume that most of the frames are referred to the correct one, and our goal is to refine the boundary and re-identify the target object in the rest of the video. Hopefully, STCN is the best candidate to carry out our goal. Our strategy is described as follows.

First, we uniformly sample K = 10 keyframes for each video sequence.
% The more keyframes we used, the more resources we consume to predict a query frame. Moreover,  
Then, these keyframes and their corresponding masks are memorized in a memory bank of the STCN\cite{cheng_rethinking_2021}. 
Finally, we re-predict the whole video by STCN and update the memory bank every 5 frames. 