\section{Preliminary}
\label{section:rel-preliminary}

\subsection{Problem Statement}
\label{subsec:rel-statement}

Video summarization aims to generate a concise overview of video content by selecting the most informative and significant parts. The resulting summary can take the form of either a set of representative video frames, known as a video storyboard, or a compilation of video fragments stitched together in chronological order, referred to as a video skim. Video skims have an advantage over static frame sets as they can include audio and motion elements, allowing for a more natural storytelling experience and potentially conveying more information. Moreover, watching a video skim is often more engaging and captivating for viewers compared to a slideshow of frames \cite{Li2001Overview}. On the other hand, storyboards offer greater flexibility in terms of data organization for browsing and navigation purposes, as they are not bound by timing or synchronization constraints \cite{Calic2007Comic,Wang2007VideoCollage}.

Our problem statement aligns closely with the concept of video storyboards, which involve selecting a subset of representative video frames to summarize the content. By focusing on these key frames, we aim to capture the essence and important aspects of the video in a condensed form. This approach allows for efficient browsing and navigation through the video data while providing a comprehensive overview of its content.

\subsection{Problem Formulation}
\label{subsec:rel-formulation}
Given an input video $\textbf{I}=\{I^{(t)}\}_{t=1}^T$ where each frame $I^{(t)} \in \mathbb{R} ^{C \times H \times W}$, the goal of video summarization using the storyboard approach is to generate a concise summary $\textbf{G}$ that consists of a subset of representative frames. The generated summary $\textbf{G}$ is denoted as $\{I^{(t_i)}\}^k_{i=1}$, where $k$ is typically much smaller than $T$ and $t_1 < t_2 < \dots < t_k$.

\subsection{Datasets}
\label{subsec:rel-datasets}

As referenced in Section \ref{section:intro-motivation}, two datasets that prevail in the video summarization bibliography are SumMe \cite{SumMe} and TVSum \cite{TVSum}. 

SumMe dataset comprises 25 videos, ranging from 1 to 6 minutes in duration, encompassing diverse content and captured from both first-person and third-person perspectives. Each video has been annotated by 15 to 18 users, resulting in multiple fragment-level user summaries. These summaries typically span 5\% to 15\% of the original video duration. 

TVSum dataset comprises 50 videos, with durations ranging from 1 to 11 minutes. These videos cover content from 10 categories of the TRECVid MED dataset. Each video in TVSum has been annotated by 20 users, providing shot- and frame-level importance scores on a scale of 1 to 5.

In addition to SumMe and TVSum, two common datasets for evaluating video summaries are OVP \cite{De2011VSUMM} and YouTube \cite{De2011VSUMM}. Each dataset comprises 50 videos, with annotations consisting of sets of key-frames generated by 5 users. The video durations span from 1 to 4 minutes for OVP and 1 to 10 minutes for YouTube. These datasets encompass a wide variety of video content, including documentaries, educational videos, ephemeral videos, historical footage, and lectures in the case of OVP, and cartoons, news clips, sports highlights, commercials, TV shows, and home videos in the case of YouTube.

Considering the size of these datasets, it is evident that there is a scarcity of large-scale annotated datasets, which limits their utility in enhancing the training of sophisticated supervised deep learning architectures.

Some less commonly used datasets for video summarization include CoSum \cite{Chu2015CoSum}, MED-summaries \cite{Potapov2014MEDSummaries}, Video Titles in the Wild (VTW) \cite{Zeng2016TitleWild}, League of Legends (LoL) \cite{Fu2017VideoLoL}, and FVPSum \cite{Ho2018FVPSum}. 

CoSum focuses on video co-summarization. It consists of 50 videos obtained from Youtube using 10 query terms related to the content of SumMe dataset. Each video has an approximate duration of 4 minutes, from which sets of key-fragments are selected by 3 different annotators.

MED-Summaries consists of 160 videos from TRECVID 2011 MED dataset. The dataset is divided into a validation set with 60 videos from 15 event categories and a test set with 100 videos from 10 event categories. The majority of videos has durations range from 1 to 5 minutes, with each being annotated with a set of importance scores, averaged over 1 to 4 annotators.

The VTW dataset consists of 18100 open domain videos, out of which 2000 videos are annotated at the sub-shot level with highlight scores. These user-generated videos are untrimmed and typically contain a highlight event. On average, the videos in the dataset have a duration of 1.5 minutes.

The LoL dataset consists of 218 long videos, ranging from 30 to 50 minutes in duration. These videos showcase game matches from the North American League of Legends Championship Series (NALCS). The annotations for this dataset are derived from a YouTube channel that features community-generated highlights, with the highlight videos typically having a duration of 5 to 7 minutes. As a result, there is one set of key-fragments available for each video in the dataset.

The FPVSum dataset focuses on first-person video summarization and comprises 98 videos, totaling over 7 hours of content. These videos are sourced from 14 categories of GoPro viewer-friendly videos. For each category, approximately 35\% of the video sequences have been annotated with ground-truth scores by at least 10 users, while the remaining sequences are considered unlabeled examples. This dataset provides valuable resources for evaluating and developing first-person video summarization algorithms.

Apostolidis \etal~\cite{Apostolidis2021Video} have compiled a comprehensive summarization table, showcasing the main characteristics of the aforementioned datasets. For reference, \hyperref[table:dataset-characteristics]{Table \ref{table:dataset-characteristics}} presents an overview of the dataset attributes, such as video count, annotation types, video duration, and user involvement.
\begin{table}
  \caption{Datasets for video summarization and their characteristics.}
  \scriptsize
  \begin{tabular}{|M{0.09\textwidth}|M{0.07\textwidth}|M{0.09\textwidth}|M{0.4\textwidth}|M{0.16\textwidth}|M{0.11\textwidth}|}
    \hline
    \bfseries Dataset & \bfseries no. videos & \bfseries duration (min) & \bfseries content & \bfseries type of annotations & \bfseries annotators per video \\ 
    [0.5ex] 
    \hline\hline
    SumMe \cite{SumMe} & 25 & 1 - 6 & holidays, events, sports & multiple sets of key-fragments & 15 - 18 \\
    \hline
    TVSum \cite{TVSum} & 50 & 2 - 10 & news, how-to's, user-generated, documentaries (10 categories - 5 video each) & multiple fragment level scores & 20 \\
    \hline
    OVP \cite{De2011VSUMM} & 50 & 1 - 4 & documentary, educational, ephemeral, historical, lecture & multiple sets of key frames & 5 \\
    \hline
    YouTube \cite{De2011VSUMM} & 50 & 1 - 10 & cartoons, sports, tv-shows, commercial. home videos & multiple sets of key frames & 5 \\
    \hline
    CoSum \cite{Chu2015CoSum} & 51 & ~ 4 & holidays, events, sports (10 categories) & multiple sets of key fragments & 3 \\
    \hline
    MED \cite{Potapov2014MEDSummaries} & 160 & 1 - 5 & 15 categories of various genres & one set of importance score & 1 - 4 \\
    \hline
    VTW \cite{Zeng2016TitleWild} & 2000 & 1.5 (avg) & user-generated videos that contain a highlight event & sub-shot level highlight scores & - \\
    \hline
    LoL \cite{Fu2017VideoLoL} & 218 & 30 - 50 & matches from a League of Legends tournament & one set of key fragments & 1 \\
    \hline
    FPVSum \cite{Ho2018FVPSum} & 98 & 4.3 (avg) & first-person videos (14 categories) & multiple frame level scores & 10 \\
    \hline
  \end{tabular}
  \label{table:dataset-characteristics}
\end{table}

% FPVSum [110] 98 4.3 (avg) first-person videos
% (14 categories) multiple frame-level scores 10
% \begin{center}
%   \begin{tabular}{|c|c|c|c|c|c|}
%   \label{table:dataset-characteristics}
%     \hline
%     Dataset & \# of videos & duration (min) & content & type of annotations & \# of annotators per video \\
%   \end{tabular}
% \end{center}
% In this study, we will thoroughly analyze and employ these datasets as benchmarks for evaluating the performance of video summarization algorithms.

\subsection{Evaluation Metrics}
\label{subsec:rel-evaluation}

	The evaluation of video summarization algorithms is a challenging task, as it is difficult to quantify the quality of a summary. In this section, we will present the multiple difficulties that prior works have encountered when evaluating video summarization algorithms. We will also give an overview the most commonly used evaluation metrics for video summarization, as well as the metrics used in this study. These information will be useful for the reader to understand the current research progress on the evaluation process of video summarization algorithms. They are provided in the subsections of \ref{subsec:rel-evaluation-difficulties} and \ref{subsec:rel-evaluation-metrics}, respectively.
	
	\subsubsection{Difficulties in Evaluation}
	\label{subsubsec:rel-evaluation-difficulties}
		Some of the most prominent problems that arise when evaluating video summarization algorithms include the lack of high-quality ground-truth summaries, the subjectivity of human perception, and the lack of a consensus on the definition of a good summary. In the following paragraphs, we will discuss some. Details about these problems are presented in \cite{Apostolidis2021Video}.

			\paragraph[short]{Lack of high-quality ground-truth summaries}
				The lack of high-quality ground-truth summaries is one of the main problems when evaluating video summarization algorithms. Ground-truth summaries are summaries that are created by humans and are used as a reference for the evaluation of automatic video summarization algorithms. However, the construction of such annotated summaries is a time-consuming and expensive process as it requires the involvement of human annotators which are inconsistent in nature. This inconsistency of human annotators means that the same evaluator may produce different summaries for the same video at different times, leading to unsure and possibly conflicting ground-truth summaries among the annotations from the same evaluator, leave alone the annotations from different evaluators as provided in the datasets from previous subsection \ref{subsec:rel-datasets}.

				Besides the inconsistency issue, the ground-truth summaries are also limited in quantity. This is because the creation of ground-truth summaries is a time-consuming and expensive process while only small number of videos were annotated with limited number of annotators in the previously published datasets.
			
			\paragraph[short]{Subjectivity of human perception}
				Different people may have different opinions on what constitutes a good summary for a given video. This subjectivity makes it difficult to evaluate the performance of an automatic video summarizer as it can lead to different ground-truth summaries for the same video which in turn creates distinct and possibly conflicting scores or opinions on the quality of a summary. Furthermore, the perceptive subjectivity also possesses problem in comparing the performance of different automatic video summarization algorithms due to several corner cases such as when an algorithm produces a summary that is judged as good by some of the human evaluators but not by others, while the other algorithm produces a summary that is judged vice versa. This problem is also known as the \textit{inter-annotator agreement} problem that is described by both \cite{measure-annotator-agreement} and \cite{inter-annotator-agreement} in details.
			
			\paragraph[short]{Lack of consensus on the definition of a good summary}
				Different people may have different opinions on what constitutes a good summary. This lack of consensus can make it difficult to evaluate the performance of automatic video summarization algorithms.

		Other than the problems that are alrady described in the previous research, there are also other problems that are not yet addressed in the evaluation process of video summarization algorithms. A notable problem that our team found during the research for prior evaluation is the possibility of several semantically different summaries that can well represent the same video. This problem is only \textit{partially} addressed in the SumMe dataset with the use of specialized aggregation method on multiple ground-truth summaries but most of this problem still persists as the number of available ground-truths is still limited.

	\subsubsection{Prior Evaluation Methods}
	\label{subsubsec:rel-evaluation-prior}
		There are several methods that have been used in literature to evaluate video summarization algorithms while addressing some of the problems mentioned in the subsection above. The two most employed methods include the approach of user studies which is the most naive and original one, and the use of ground-truth summaries as references for computation of objective metrics. Details about these methods are presented in \cite{Apostolidis2021Video} and we provide a brief overview of them in the following paragraphs.

		\paragraph[short]{User studies}
			The most naive and original method for evaluating video summarization algorithms is to conduct user studies. In this method, the performance of an algorithm is evaluated by asking human evaluators to watch the video summaries produced by the algorithm and then rate the quality of the summaries. The quality of a summary is usually rated by the evaluators based on their subjective opinions. This method is the most naive and original one because it is the most straightforward way to evaluate the performance of an algorithm. Furthermore, it is also the most expensive and time-consuming method as it requires the involvement of human evaluators. Besides such disadvantages, this method is also the least accurate one as the human evaluators are inherently inconsistent in nature. This inconsistency of human evaluators means that the same evaluator may produce distinct scores for the same summary at different times, making such evaluation process not possible to be reproduced in the future. Therefore, the current literature has moved away from this method for easier reproducibility as well as consistency and low-cost evaluation of their methods. More details can be found in the survey by Apostolidis et al. \cite{Apostolidis2021Video}.

		\paragraph[short]{Objective metrics}
			Another method that has been increasingly used in literature to evaluate automatically generated summaries is the use of artificially annotated ground-truths as references for computation of objective metrics. In this method, the performance of an algorithm is evaluated by comparing the summary produced by the algorithm with the pre-defined ground-truth summaries created by human evalutors. The comparison is usually done by computing the similarity between the summary produced by the algorithm and the ground-truth summaries. The similarity between the summary produced by the algorithm and the ground-truth summaries is usually computed with objective metrics such as accuracy and error rates which were proposed by \cite{ejaz2012adaptive} and adopted by \cite{almeida2012vison,cahuina2013new,jacob2017video}, or the more well-known precision, recall, and F-measure that were published by \cite{mahmoud2013unsupervised} and used by \cite{gong2014diverse,guan2014top,mei2015video,demir2015video}. This method is less expensive and time-consuming than the user studies method as it does not require the involvement of human evaluators.

			As all of the previously mentioned objective metrics are computed based on a fundamental assumption that all summaries, either automatically generated or artificially annotated, comprise of keyframes selected from the video content, their resulting performance measures are not soft enough to finely rank the performane of different methods performing summarization on the same video. This is because the keyframes selected from the video content are not always the same as the keyframes selected by human evaluators. 
			
			A notable variation of this method was introduced in the SumMe dataset \cite{SumMe} and adopted by TVSum dataset \cite{TVSum}, which 
			
			However, it is also less accurate than the user studies method as it is based on the assumption that the ground-truth summaries are accurate representations of the videos. This assumption is not always true as the ground-truth summaries are usually created by human evaluators which are inconsistent in nature. This inconsistency of human evaluators means that the same evaluator may produce different summaries for the same video at different times, leading to unsure and possibly conflicting ground-truth summaries among the annotations from the same evaluator, leave alone the annotations from different evaluators as provided in the datasets from previous subsection \ref{subsec:rel-datasets}.

		
		To the best of our knowledge, there is currently no prior work that has fully addressed this problem and hence, solving