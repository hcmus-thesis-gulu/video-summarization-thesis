\begin{EnAbstract}
  Video summarization is an emerging research field that addresses the need for efficient video browsing and retrieval in today's vast and ever-expanding video collections. With the exponential growth of multimedia data, the ability to effectively analyze and extract relevant information from video content has become crucial. Video summarization techniques aim to automatically generate a concise and meaningful representation of a video by selecting key frames, shots, or segments that capture the essence of the content. This process can significantly reduce the time and effort required to review and analyze video data, thereby improving the efficiency and accuracy of various applications, including video surveillance, education, entertainment, and social media.
  
  Despite the wide-ranging usage of video summarization, there are only a few datasets available for this task, with the two most prominent are SumMe \cite{SumMe} and TVSum \cite{TVSum}. This limitation hinders the comprehensive evaluation and benchmarking of video summarization algorithms. The scarcity of diverse and representative datasets restricts the generalizability and effectiveness of developed techniques. Additionally, the evaluation metrics employed for video summarization are also flawed, as they fail to fully capture the inherent challenges and complexities involved in generating high-quality video summaries. This inadequacy hampers the accurate assessment of different algorithms and inhibits the advancement of the field.

  However, the inherent nature of the video summarization task poses challenges in evaluating the quality of generated summaries without human involvement. It is difficult to determine objectively whether one video summary is superior to another without relying on subjective human judgment. Recognizing this limitation, we propose a self-supervised model that mitigates the issues associated with the data-intensive nature of video summarization. By moving away from fixed ground truth annotations and instead leveraging the inherent structure and information within the video data itself, our self-supervised model learns to generate informative and representative summaries.

  In addition to addressing the data scarcity challenge, we also introduce an innovative evaluation pipeline specifically tailored for the video summarization task. To ensure that our generated summaries effectively capture the essence of the original videos, we conduct a comprehensive survey involving human participants. The survey participants are provided with the original videos, ground truth summaries, and our generated summaries. They are then asked to evaluate and compare the informativeness of the generated summaries against the ground truth summaries. This human-centric evaluation approach enables us to obtain valuable insights into the performance and effectiveness of our proposed video summarization techniques.

  By proposing a self-supervised model and an evaluation pipeline that incorporates human judgment, this thesis not only addresses the data scarcity and evaluation challenges but also provides a more realistic and meaningful assessment of the video summarization task. The experimental results and feedback obtained from the survey validate the efficacy and relevance of our proposed approaches, highlighting their potential for improving the accuracy and reliability of video summarization in practical applications.
\end{EnAbstract}