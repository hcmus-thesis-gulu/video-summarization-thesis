\subsection{Supervision on frame importance with video spatiotemporal structure}
\label{subsec:rel-sup-spatiotemporal}

In order to improve the estimation of video frame/fragment importance, certain techniques focus on capturing both the spatial and temporal structure of the video. These approaches not only take into account the input sequence of video frames and the available ground-truth data indicating frame importance, but also model the spatiotemporal dependencies among frames. This additional analysis enhances the training process of the Summarizer, as shown by the dashed rectangles and lines in \hyperref[figure:rel-sup-model]{Figure \ref{figure:rel-sup-model}}. Lal \etal~\cite{lal2019online} introduced an encoder-decoder architecture with convolutional LSTMs that effectively models the spatiotemporal relationships within the video. The algorithm not only estimates frame importance but also enhances visual diversity through next frame prediction and shot detection mechanisms, leveraging the likelihood that initial frames of a shot are often part of the summary. Yuan \etal~\cite{yuan2019spatiotemporal} employed a trainable 3D-CNN to extract deep and shallow features from the video content and fused them to create a new representation. This representation, combined with convolutional LSTMs, captures the spatial and temporal structure of the video. A novel loss function called Sobolev loss is then used to learn summarization by minimizing the distance between the series of frame-level importance scores and the ground-truth scores, effectively exploiting the temporal structure of the video. Chu \etal~\cite{chu2019spatiotemporal} leveraged CNNs to extract spatial and temporal information from raw frames and optical flow maps. Through a label distribution learning process, they learned to estimate frame importance based on human annotations.  Elfeki \etal~\cite{elfeki2019video} combined CNNs and Gated Recurrent Units (GRUs), a type of RNN, to form spatiotemporal feature vectors. These vectors were used to estimate the level of activity and importance for each frame. Huang \etal~\cite{huang2019novel} trained a neural network to extract spatiotemporal information from the video, specifically focusing on inter-frame motion. This information was used to create an inter-frames motion curve, which was then input into a transition effects detection method for shot segmentation. A self-attention model, guided by human-generated ground-truth data, was employed to estimate intra-shot importance and select key frames/fragments for creating static/dynamic video summaries. By incorporating the spatial and temporal aspects of videos, these supervised approaches improve the accuracy of frame importance estimation and enable the generation of more informative video summaries.
