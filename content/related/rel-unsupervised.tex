\section{Unsupervised approaches}
\label{section:rel-unsupervised}

Unsupervised methods eliminate the need for ground-truth data, which typically requires time-consuming and labor-intensive manual annotation. Instead, unsupervised approaches leverage large collections of original videos for training. Through learning mechanisms designed for unsupervised settings, these methods extract meaningful information from the video data to generate summaries.

% \section{Overview}
% \label{sec:overview}

% \subsection{Introduction to Lifelog Retrieval}

% In 2021, we witnessed the rising popularity of video content on platforms such as TikTok, Instagram Reels, and YouTube Shorts. Along with the promise about the metaverse, they show that content is moving from simple images to more complex forms, such as video and virtual reality. A system that can handle these new forms of data, which is more costly computational and storage-wise, can undoubtedly provide great value. While the LSC'22 dataset \cite{gurrin_introduction_2022} is not a video dataset on its own, it is similar to one, in terms of size and temporal meaning. It is greater than the previous edition (roughly 725,000 images compared to 183,299 \cite{gurrin_introduction_2021}) and poses a significant challenge to system developers \cite{gurrin_introduction_2022}.

% In recent years, Transformer \cite{vaswani_attention_2017} has become the prevalent architecture in both text and image domains. They have inspired the use of large collections of unlabeled data in training, which is easier to obtain. The approach is sometimes called self-supervised learning. When large image-text or video-text datasets become available, they give rise to vision-language pre-training. This approach creates large multi-purpose image-text models pre-trained on matching images to their captions instead of performing a specific task such as classification. These models are applicable to many downstream tasks such as Video-Text Retrieval \cite{gao_clip2tv_2021}, or even zero-shot Video Retrieval \cite{portillo-quintero_straightforward_2021}. With the nature of being trained on image-text data, we believe that they are especially suitable for a cross-domain image-text retrieval task, which turns out to be exactly what LSC is.

% Since the LSC competition centers around interactivity, this means that good systems also have practical value. In addition, they also hold a novice session to make sure the systems are easy to use, even for non-expert users. With that in mind, we seek to use the large vision-language pre-trained models' representational strength and versatility to empower our search engine while at the same time simplifying user interaction through a better understanding of image sequence semantics.

% %Short summary about recent methods
% % Providing effective access methodologies for personal lifelogs can trace its beginning back to the seminal MyLifeBits system \cite{Gemmel-2006} which indexed Gordon Bell's lifetime of digital data. To motivate and facilitate the development of more effective lifelog retrieval systems, the organizers of the Lifelog Search Challenge (LSC) launched this series of annual challenges in 2018 as a comparative benchmarking workshop with the aim of fostering scalable and effective retrieval technologies for large lifelog datasets. The LSC workshop provided participants with a large lifelog dataset and a set of topics to solve. Competing teams must find an image from the lifelog archive that best addresses the information need posed by the topic within a limited time period. In this paper, we introduce LSC'22, the fifth iteration of the Lifelog Search Challenge. %, LSC'21.

% Based on FIRST 2.0 \cite{trang-trung_flexible_2021}, we revise and improve the functionalities and performance of our retrieval system, as well as integrate new components to FIRST 3.0. First, we enhance the semantic encoding for an image using CLIP \cite{radford_learning_2021}. We propose representing an image by a set of adaptive semantic embedding vectors, each corresponding to either the whole image or various regions of interest in different sizes. In this way, our system is expected to better capture the semantics of an image to search for concepts at varying levels of granularity. Second, we propose augmenting our system with an external search engine, such as Google, to find visual examples corresponding to unfamiliar concepts for our system to retrieve visually similar moments in the collection of images. Third, because of the vast amount of images, we utilize the clustering of images to shots, \textit{i.e. }sequence of contiguous similar images, and scenes, \textit{i.e.} similar shots in the same place at different time instants, to organize images in hierarchical clusters for efficient exploration.

% \subsection{Introduction to Referring Expression Segmentation}
% \vspace{-2mm}
% Referring expression segmentation aims to predict a pixel-wise mask of the referred object given an image and a natural language expression. This task can be potentially used in a wide range of applications, including human-object interaction and image editing. Unlike traditional visual segmentation tasks (such as semantic segmentation \cite{he_adaptive_2019} and instance segmentation \cite{he_mask_2017}) that require a fixed number of categories, referring expression segmentation has to deal with a broader amount of vocabularies and syntax diversities of human languages. In this task, the target object is mentioned with various forms of expression, such as words, phrases, or complex sentences presenting the concepts of actions, positions, objects, etc. Hence, the most challenging part of this task is to understand the expression and highlight the regions that are relevant to that expression.

% \vspace{-2mm}
% Over the past few years, the referring expression segmentation task has grown rapidly. Early approaches \cite{liu_recurrent_2017, margffoy-tuay_dynamic_2018, li_referring_2018} leverage a fusion module between visual and linguistic features and followed by a cross-modal decoder to generate masks of the referred object. Concretely, the fusion modules includes recurrent interaction \cite{liu_recurrent_2017, li_referring_2018}, cross-modal attention \cite{shi_key-word-aware_2018, chen_see-through-text_2019}, language-guided modeling \cite{huang_referring_2020, hui_linguistic_2020}, etc. Recently, Transformer \cite{vaswani_attention_2017} shows a significant improvement in performance of cross-modal alignments \cite{ding_vision-language_2021,yang_lavt_2022, botach_end--end_2022} (illustrated in Fig. \ref{fig:comparison_pipeline}). 



% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.7\linewidth]{content/resources/images/referring_segmentation/CompareOverview.pdf}
%     \caption{Comparison of referring image segmentation pipelines. (a) The previous state-of-the-art approach (i.e., LAVT \cite{yang_lavt_2022}) integrate linguistic features into visual features of a vision transformer model to benefit the jointly exploiting vision-language cues. (b) We propose to leverage a Transformer-based module to associate both visual and linguistic information with a set of object queries, aiming to gradually update the representation of these object queries. The final object queries then produce the mask prediction.}
%     \label{fig:comparison_pipeline}
% \end{figure}

% \vspace{-2mm}
% In most previous transformer-based works in computer vision \cite{carion_end--end_2020, wang_end--end_2021, cheng_per-pixel_2021, cheng_masked-attention_2022}, a set of queries is used to represent the class or instance features for detection and segmentation. In referring segmentation task, some works \cite{ding_vision-language_2021, wu_language_2022} generate the query vectors from language features using vision-guided attention or directly. Then these queries are updated in a Transformer decoder using visual features only. It can lead to a problem that the linguistic information can vanish in the object query features after several Transformer decoder layers. To address this issue, a potential solution is to exploit a multi-modal Transformer for simultaneously aggregating visual and linguistic features during the transformer decoder.

% Besides, on such referring segmentation problems, the text query usually contains information about the category, position on the image, and appearance of the object. In some cases, the related position between the referred object and others is mentioned. CLIP \cite{radford_learning_2021} models are learned from a wide range of visual concepts followed by the natural language. Hence, these models can better capture the information related to the visual concepts. 


% Hence, we propose a Visual-Linguistic Transformers (VLFormer) \citeown{nguyen_vlformer_2022} approach to leverage a set of queries that understand both visual and linguistic features to represent potential objects. First, the CLIP Text Encoder is utilized for linguistic features from natural language expression. The linguistic features are used to improve the vision-language fusion and enhance the representation of object queries through the Transformer-based module. Second, a Visual-Linguistic Transformer, which contains several Visual-Linguistic Transformer Block modules, is designed for constructing fine-grained object features using linguistic and multi-scale visual information. Each Visual-Linguistic Transformer Block (VLB) utilizes the cross-attention modules from linguistic and visual features to object queries, then generates more informative object queries. Figure~\ref{fig:comparison_pipeline} highlights the difference between our proposed work and the state-of-the-art method.