\section{Preliminary}
\label{section:rel-preliminary}

\subsection{Problem Statement}
\label{subsec:rel-statement}

Video summarization aims to generate a concise overview of video content by selecting the most informative and significant parts. The resulting summary can take the form of either a set of representative video frames, known as a video storyboard, or a compilation of video fragments stitched together in chronological order, referred to as a video skim. Video skims have an advantage over static frame sets as they can include audio and motion elements, allowing for a more natural storytelling experience and potentially conveying more information. Moreover, watching a video skim is often more engaging and captivating for viewers compared to a slideshow of frames \cite{Li2001Overview}. On the other hand, storyboards offer greater flexibility in terms of data organization for browsing and navigation purposes, as they are not bound by timing or synchronization constraints \cite{Calic2007Comic,Wang2007VideoCollage}.

Our problem statement aligns closely with the concept of video storyboards, which involve selecting a subset of representative video frames to summarize the content. By focusing on these key frames, we aim to capture the essence and important aspects of the video in a condensed form. This approach allows for efficient browsing and navigation through the video data while providing a comprehensive overview of its content.

\subsection{Problem Formulation}
\label{subsec:rel-formulation}
Given an input video $\textbf{I}=\{I^{(t)}\}_{t=1}^T$ where each frame $I^{(t)} \in \mathbb{R} ^{C \times H \times W}$, the goal of video summarization using the storyboard approach is to generate a concise summary $\textbf{S}$ that consists of a subset of representative frames. The summary $\textbf{S}$ is denoted as $\{I^{(t_i)}\}^k_{i=1}$, where $k$ is typically much smaller than $T$ and $t_1 < t_2 < \dots < t_k$.

\subsection{Datasets}
\label{subsec:rel-datasets}

As referenced in Section \ref{section:intro-motivation}, two datasets that prevail in the video summarization bibliography are SumMe \cite{Gygli2014SumMe} and TVSum \cite{Song2015TVSum}. 

SumMe dataset comprises 25 videos, ranging from 1 to 6 minutes in duration, encompassing diverse content and captured from both first-person and third-person perspectives. Each video has been annotated by 15 to 18 users, resulting in multiple fragment-level user summaries. These summaries typically span 5\% to 15\% of the original video duration. 

TVSum dataset comprises 50 videos, with durations ranging from 1 to 11 minutes. These videos cover content from 10 categories of the TRECVid MED dataset. Each video in TVSum has been annotated by 20 users, providing shot- and frame-level importance scores on a scale of 1 to 5.

In addition to SumMe and TVSum, two common datasets for evaluating video summaries are OVP \cite{De2011VSUMM} and YouTube \cite{De2011VSUMM}. Each dataset comprises 50 videos, with annotations consisting of sets of key-frames generated by 5 users. The video durations span from 1 to 4 minutes for OVP and 1 to 10 minutes for YouTube. These datasets encompass a wide variety of video content, including documentaries, educational videos, ephemeral videos, historical footage, and lectures in the case of OVP, and cartoons, news clips, sports highlights, commercials, TV shows, and home videos in the case of YouTube.

Considering the size of these datasets, it is evident that there is a scarcity of large-scale annotated datasets, which limits their utility in enhancing the training of sophisticated supervised deep learning architectures.

Some less commonly used datasets for video summarization include CoSum \cite{Chu2015CoSum}, MED-summaries \cite{Potapov2014MEDSummaries}, Video Titles in the Wild (VTW) \cite{Zeng2016TitleWild}, League of Legends (LoL) \cite{Fu2017VideoLoL}, and FVPSum \cite{Ho2018FVPSum}. 

CoSum focuses on video co-summarization. It consists of 50 videos obtained from Youtube using 10 query terms related to the content of SumMe dataset. Each video has an approximate duration of 4 minutes, from which sets of key-fragments are selected by 3 different annotators.


The MED-summaries
dataset contains 160 annotated videos from the TRECVID
2011 MED dataset. 60 videos form the validation set (from
15 event categories) and the remaining 100 videos form the
test set (from 10 event categories), with most of them being
1 to 5 minutes long. The annotations come as one set of
importance scores, averaged over 1 to 4 annotators

% In this study, we will thoroughly analyze and employ these datasets as benchmarks for evaluating the performance of video summarization algorithms.

\subsection{Evaluation Metrics}
\label{subsec:rel-evaluation}