\section{Motivation}
\label{section:intro-motivation}

Despite the wide-ranging usage of video summarization, there are only a few datasets available for this task, with the two most prominent being SumMe \cite{SumMe} and TVSum \cite{TVSum}. This limitation hinders the comprehensive evaluation and benchmarking of video summarization algorithms. The scarcity of diverse and representative datasets restricts the generalizability and effectiveness of developed techniques.

The nature of video summarization task poses a challenge for supervised approaches. Traditional metrics, such as F-measure and precision-recall curves, rely heavily on frame-level matching and do not adequately account for the temporal coherence and semantic understanding of the summary. These kind of metrics fail to fully capture the inherent challenges and complexities involved in generating high-quality video summaries.

Recognize the difficulty of evaluating video summaries solely based on fix ground truths, we propose an innovative evaluation pipeline tailored specifically for the video summarization task. In order to ensure that our generated summaries effectively capture the essence of the original videos, we conduct a comprehensive survey involving human participants. The survey participants are provided with the original videos, ground truth summaries, and our generated summaries. They are then asked to evaluate and compare the informativeness of the generated summaries against the ground truth summaries. This human-centric evaluation approach allows for a more realistic and meaningful assessment of our proposed video summarization techniques.

In addition to the novel human-based evaluation metric, this thesis introduces a self-supervised model that overcomes the challenges associated with the data-intensive nature of video summarization. Instead of relying on supervision with ground truth annotations, our model leverages the inherent structure and information within the video data itself to generate informative and representative summaries. By moving away from the limitations of traditional annotation-based approaches, our self-supervised model aims to enhance the quality and generalizability of video summarization techniques.