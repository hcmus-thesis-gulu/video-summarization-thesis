
@inproceedings{vondrick_tracking_2018,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Tracking {Emerges} by {Colorizing} {Videos}},
	volume = {11217},
	url = {https://doi.org/10.1007/978-3-030-01261-8\_24},
	doi = {10.1007/978-3-030-01261-8_24},
	booktitle = {Computer {Vision} - {ECCV} 2018 - 15th {European} {Conference}, {Munich}, {Germany}, {September} 8-14, 2018, {Proceedings}, {Part} {XIII}},
	publisher = {Springer},
	author = {Vondrick, Carl and Shrivastava, Abhinav and Fathi, Alireza and Guadarrama, Sergio and Murphy, Kevin},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	pages = {402--419},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\ZT4RASLQ\\Vondrick et al. - 2018 - Tracking Emerges by Colorizing Videos.pdf:application/pdf},
}

@inproceedings{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	url = {https://doi.org/10.18653/v1/n19-1423},
	doi = {10.18653/v1/n19-1423},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL}-{HLT} 2019, {Minneapolis}, {MN}, {USA}, {June} 2-7, 2019, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	year = {2019},
	pages = {4171--4186},
}

@article{dosovitskiy_discriminative_2016,
	title = {Discriminative {Unsupervised} {Feature} {Learning} with {Exemplar} {Convolutional} {Neural} {Networks}},
	volume = {38},
	url = {https://doi.org/10.1109/TPAMI.2015.2496141},
	doi = {10.1109/TPAMI.2015.2496141},
	number = {9},
	journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	author = {Dosovitskiy, Alexey and Fischer, Philipp and Springenberg, Jost Tobias and Riedmiller, Martin A. and Brox, Thomas},
	year = {2016},
	pages = {1734--1747},
}

@article{oord_representation_2018,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	volume = {abs/1807.03748},
	url = {http://arxiv.org/abs/1807.03748},
	journal = {CoRR},
	author = {Oord, Aäron van den and Li, Yazhe and Vinyals, Oriol},
	year = {2018},
	note = {arXiv: 1807.03748},
}

@inproceedings{he_momentum_2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	url = {https://doi.org/10.1109/CVPR42600.2020.00975},
	doi = {10.1109/CVPR42600.2020.00975},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2020, {Seattle}, {WA}, {USA}, {June} 13-19, 2020},
	publisher = {Computer Vision Foundation / IEEE},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross B.},
	year = {2020},
	pages = {9726--9735},
}

@inproceedings{radford_learning_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	volume = {139},
	url = {http://proceedings.mlr.press/v139/radford21a.html},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}, {ICML} 2021, 18-24 {July} 2021, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	editor = {Meila, Marina and Zhang, Tong},
	year = {2021},
	pages = {8748--8763},
	file = {Full Text:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\AXJGQKW7\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf},
}

@inproceedings{jia_scaling_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Scaling {Up} {Visual} and {Vision}-{Language} {Representation} {Learning} {With} {Noisy} {Text} {Supervision}},
	volume = {139},
	url = {http://proceedings.mlr.press/v139/jia21b.html},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}, {ICML} 2021, 18-24 {July} 2021, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V. and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
	editor = {Meila, Marina and Zhang, Tong},
	year = {2021},
	pages = {4904--4916},
}

@inproceedings{chen_uniter_2020,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{UNITER}: {UNiversal} {Image}-{TExt} {Representation} {Learning}},
	volume = {12375},
	url = {https://doi.org/10.1007/978-3-030-58577-8\_7},
	doi = {10.1007/978-3-030-58577-8_7},
	booktitle = {Computer {Vision} - {ECCV} 2020 - 16th {European} {Conference}, {Glasgow}, {UK}, {August} 23-28, 2020, {Proceedings}, {Part} {XXX}},
	publisher = {Springer},
	author = {Chen, Yen-Chun and Li, Linjie and Yu, Licheng and Kholy, Ahmed El and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {104--120},
}

@inproceedings{anderson_bottom-up_2018,
	title = {Bottom-{Up} and {Top}-{Down} {Attention} for {Image} {Captioning} and {Visual} {Question} {Answering}},
	booktitle = {The {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE Computer Society},
	author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
	year = {2018},
	keywords = {attention caption image paper-dzhi q\&a},
	pages = {6077--6086},
}

@inproceedings{rao_denseclip_2022,
	title = {{DenseCLIP}: {Language}-{Guided} {Dense} {Prediction} {With} {Context}-{Aware} {Prompting}},
	shorttitle = {{DenseCLIP}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-06},
	author = {Rao, Yongming and Zhao, Wenliang and Chen, Guangyi and Tang, Yansong and Zhu, Zheng and Huang, Guan and Zhou, Jie and Lu, Jiwen},
	year = {2022},
	pages = {18082--18091},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\35MMKWLJ\\Rao et al. - 2022 - DenseCLIP Language-Guided Dense Prediction With C.pdf:application/pdf;Snapshot:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\B73UTQRY\\Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.html:text/html},
}

@article{liang_rethinking_nodate,
	title = {Rethinking {Cross}-modal {Interaction} from a {Top}-down {Perspective} for {Referring} {Video} {Object} {Segmentation}},
	abstract = {Referring video object segmentation (RVOS) aims to segment video objects with the guidance of natural language reference. Previous methods typically tackle RVOS through directly grounding linguistic reference over the image lattice. Such bottom-up strategy fails to explore object-level cues, easily leading to inferior results. In this work, we instead put forward a two-stage, top-down RVOS solution. First, an exhaustive set of object tracklets is constructed by propagating object masks detected from several sampled frames to the entire video. Second, a Transformer-based tracklet-language grounding module is proposed, which models instance-level visual relations and cross-modal interactions simultaneously and efﬁciently. Our model ranks 1st place on CVPR2021 Referring Youtube-VOS challenge.},
	language = {en},
	author = {Liang, Chen and Wu, Yu and Zhou, Tianfei and Wang, Wenguan and Yang, Zongxin and Wei, Yunchao and Yang, Yi},
	pages = {4},
	file = {Liang et al. - Rethinking Cross-modal Interaction from a Top-down.pdf:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\KJNRIJ7M\\Liang et al. - Rethinking Cross-modal Interaction from a Top-down.pdf:application/pdf},
}

@article{ding_progressive_nodate,
	title = {Progressive {Multimodal} {Interaction} {Network} for {Referring} {Video} {Object} {Segmentation}},
	abstract = {Referring video object segmentation aims to segment the target object in the video referred by a natural language description. Existing methods perform the coarse late multimodal fusion to align visual and linguistic modalities, identifying the referent matched with the description. Then the memory attention among frames is conducted to reﬁne the results in other frames. To achieve ﬁner multimodal feature fusion, we propose a Progressive Multimodal Interaction Network (PMINet) which performs multimodal feature fusion in each stage of visual backbone, enabling the progressive learning of visual features under the guidance of linguistic features. Afterwards, we conduct other postprocessing techniques to reﬁne the mask prediction among all the frames, yielding the temporal consistency of segmentation result of the whole video. Our proposed method achieves the second place on the Track 3: Referring Video Object Segmentation of the 2021 YouTube VOS Challenge.},
	language = {en},
	author = {Ding, Zihan and Hui, Tianrui and Huang, Shaofei and Liu, Si and Luo, Xuan and Huang, Junshi and Wei, Xiaoming},
	pages = {4},
	file = {Ding et al. - Progressive Multimodal Interaction Network for Ref.pdf:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\3NEU7VPB\\Ding et al. - Progressive Multimodal Interaction Network for Ref.pdf:application/pdf},
}

@inproceedings{wu_language_2022,
	title = {Language {As} {Queries} for {Referring} {Video} {Object} {Segmentation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Wu_Language_As_Queries_for_Referring_Video_Object_Segmentation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-13},
	author = {Wu, Jiannan and Jiang, Yi and Sun, Peize and Yuan, Zehuan and Luo, Ping},
	year = {2022},
	pages = {4974--4984},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\F6ICFMI7\\Wu et al. - 2022 - Language As Queries for Referring Video Object Seg.pdf:application/pdf},
}

@inproceedings{botach_end--end_2022,
	title = {End-to-{End} {Referring} {Video} {Object} {Segmentation} {With} {Multimodal} {Transformers}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Botach_End-to-End_Referring_Video_Object_Segmentation_With_Multimodal_Transformers_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-13},
	author = {Botach, Adam and Zheltonozhskii, Evgenii and Baskin, Chaim},
	year = {2022},
	pages = {4985--4995},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\EGTZPDDA\\Botach et al. - 2022 - End-to-End Referring Video Object Segmentation Wit.pdf:application/pdf},
}

@inproceedings{cheng_masked-attention_2022,
	title = {Masked-{Attention} {Mask} {Transformer} for {Universal} {Image} {Segmentation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Cheng_Masked-Attention_Mask_Transformer_for_Universal_Image_Segmentation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-13},
	author = {Cheng, Bowen and Misra, Ishan and Schwing, Alexander G. and Kirillov, Alexander and Girdhar, Rohit},
	year = {2022},
	pages = {1290--1299},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\V7T74JH2\\Cheng et al. - 2022 - Masked-Attention Mask Transformer for Universal Im.pdf:application/pdf;Snapshot:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\DCBHUHDD\\Cheng_Masked-Attention_Mask_Transformer_for_Universal_Image_Segmentation_CVPR_2022_paper.html:text/html},
}

@inproceedings{wang_cris_2022,
	title = {{CRIS}: {CLIP}-{Driven} {Referring} {Image} {Segmentation}},
	shorttitle = {{CRIS}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Wang_CRIS_CLIP-Driven_Referring_Image_Segmentation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-13},
	author = {Wang, Zhaoqing and Lu, Yu and Li, Qiang and Tao, Xunqiang and Guo, Yandong and Gong, Mingming and Liu, Tongliang},
	year = {2022},
	pages = {11686--11695},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\KZKTY7V4\\Wang et al. - 2022 - CRIS CLIP-Driven Referring Image Segmentation.pdf:application/pdf;Snapshot:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\YCEZXSCL\\Wang_CRIS_CLIP-Driven_Referring_Image_Segmentation_CVPR_2022_paper.html:text/html},
}

@inproceedings{gurrin_introduction_2022,
	address = {New York, NY, USA},
	series = {{ICMR} '22},
	title = {Introduction to the {Fifth} {Annual} {Lifelog} {Search} {Challenge}, {LSC}'22},
	isbn = {978-1-4503-9238-9},
	url = {https://doi.org/10.1145/3512527.3531439},
	doi = {10.1145/3512527.3531439},
	abstract = {For the fifth time since 2018, the Lifelog Search Challenge (LSC) facilitated a benchmarking exercise to compare interactive search systems designed for multimodal lifelogs. LSC'22 attracted nine participating research groups who developed interactive lifelog retrieval systems enabling fast and effective access to lifelogs. The systems competed in front of a hybrid audience at the LSC workshop at ACM ICMR'22. This paper presents an introduction to the LSC workshop, the new (larger) dataset used in the competition, and introduces the participating lifelog search systems.},
	urldate = {2022-07-13},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Gurrin, Cathal and Zhou, Liting and Healy, Graham and Þór Jónsson, Björn and Dang-Nguyen, Duc-Tien and Lokoć, Jakub and Tran, Minh-Triet and Hürst, Wolfgang and Rossetto, Luca and Schöffmann, Klaus},
	month = jun,
	year = {2022},
	keywords = {lifelog, benchmarking, interactive retrieval systems},
	pages = {685--687},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\4G79BDN8\\Gurrin et al. - 2022 - Introduction to the Fifth Annual Lifelog Search Ch.pdf:application/pdf},
}

@inproceedings{alam_memento_2022,
	address = {New York, NY, USA},
	series = {{LSC} '22},
	title = {Memento 2.0: {An} {Improved} {Lifelog} {Search} {Engine} for {LSC}'22},
	isbn = {978-1-4503-9239-6},
	shorttitle = {Memento 2.0},
	url = {https://doi.org/10.1145/3512729.3533006},
	doi = {10.1145/3512729.3533006},
	abstract = {In this paper, we present Memento 2.0, an improved version of our system which first participated in the Lifelog Search Challenge 2021. Memento 2.0 employs image-text embeddings derived from two CLIP models (ViT-L/14 and ResNet-50x64) and adopts a weighted ensemble approach to derive a combined final ranking. Our approach significantly improves the performance over the baseline LSC'21 system. We additionally make important updates to the system's user interface after analysing the shortcomings to make it more efficient and better suited to the needs of the Lifelog Search Challenge.},
	urldate = {2022-07-13},
	booktitle = {Proceedings of the 5th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Alam, Naushad and Graham, Yvette and Gurrin, Cathal},
	month = jun,
	year = {2022},
	keywords = {information systems, retrieval models and ranking, search interfaces},
	pages = {2--7},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\4GW3ZQJP\\Alam et al. - 2022 - Memento 2.0 An Improved Lifelog Search Engine for.pdf:application/pdf},
}

@inproceedings{nguyen_lifeseeker_2022,
	address = {New York, NY, USA},
	series = {{LSC} '22},
	title = {{LifeSeeker} 4.0: {An} {Interactive} {Lifelog} {Search} {Engine} for {LSC}'22},
	isbn = {978-1-4503-9239-6},
	shorttitle = {{LifeSeeker} 4.0},
	url = {https://doi.org/10.1145/3512729.3533014},
	doi = {10.1145/3512729.3533014},
	abstract = {In this paper, we introduce LifeSeeker 4.0 - an interactive lifelog retrieval system developed for the fifth annual Lifelog Search Challenge (LSC'22). In LifeSeeker 4.0, we focus on enhancing our previous system to allow users who have little to no knowledge of underlying system functioning and lifelog data to use it with ease by employing a Contrastive Language-Image Pre-training (CLIP) model. Furthermore, we have exploited the music metadata to facilitate searches that may incorporate emotion. Event clustering is also improved in this version to increase user experience by reducing the occurrence of repeated images, and hence decreasing the search time.},
	urldate = {2022-07-13},
	booktitle = {Proceedings of the 5th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Thao-Nhu and Le, Tu-Khiem and Ninh, Van-Tu and Tran, Minh-Triet and Nguyen, Thanh Binh and Healy, Graham and Smyth, Sinéad and Caputo, Annalina and Gurrin, Cathal},
	month = jun,
	year = {2022},
	keywords = {lifelog, information system, interactive retrieval},
	pages = {14--19},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\Y59NLRBF\\Nguyen et al. - 2022 - LifeSeeker 4.0 An Interactive Lifelog Search Engi.pdf:application/pdf},
}

@inproceedings{hoang-xuan_flexible_2022,
	address = {New York, NY, USA},
	series = {{LSC} '22},
	title = {Flexible {Interactive} {Retrieval} {SysTem} 3.0 for {Visual} {Lifelog} {Exploration} at {LSC} 2022},
	isbn = {978-1-4503-9239-6},
	url = {https://doi.org/10.1145/3512729.3533013},
	doi = {10.1145/3512729.3533013},
	abstract = {Building a retrieval system with lifelogging data is more complicated than with ordinary data due to the redundancies, blurriness, massive amount of data, various sources of information accompanying lifelogging data, and especially the ad-hoc nature of queries. The Lifelog Search Challenge (LSC) is a benchmarking challenge that encourages researchers and developers to push the boundaries in lifelog retrieval. For LSC'22, we develop FIRST 3.0, a novel and flexible system that leverages expressive cross-domain embeddings to enhance the searching process. Our system aims to adaptively capture the semantics of an image at different levels of detail. We also propose to augment our system with an external search engine to help our system with initial visual examples for unfamiliar concepts. Finally, we organize image data in hierarchical clusters based on their visual similarity and location to assist users in data exploration. Experiments show that our system is both fast and effective in handling various retrieval scenarios.},
	urldate = {2022-07-13},
	booktitle = {Proceedings of the 5th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Hoang-Xuan, Nhat and Trang-Trung, Hoang-Phuc and Nguyen, E-Ro and Le, Thanh-Cong and Tran, Mai-Khiem and Le, Tu-Khiem and Ninh, Van-Tu and Gurrin, Cathal and Tran, Minh-Triet},
	month = jun,
	year = {2022},
	keywords = {lifelog, interactive retrieval systems, query expansion, semantic embedding},
	pages = {20--26},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\S8I67VRN\\Hoang-Xuan et al. - 2022 - Flexible Interactive Retrieval SysTem 3.0 for Visu.pdf:application/pdf},
}

@inproceedings{tran_e-mysce_2022,
	address = {New York, NY, USA},
	series = {{LSC} '22},
	title = {E-{Myscéal}: {Embedding}-based {Interactive} {Lifelog} {Retrieval} {System} for {LSC}'22},
	isbn = {978-1-4503-9239-6},
	shorttitle = {E-{Myscéal}},
	url = {https://doi.org/10.1145/3512729.3533012},
	doi = {10.1145/3512729.3533012},
	abstract = {Developing interactive lifelog retrieval systems is a growing research area. There are many international competitions for lifelog retrieval that encourage researchers to build effective systems that can address the multimodal retrieval challenge of lifelogs. The Lifelog Search Challenge (LSC) was first organised in 2018 and is currently the only interactive benchmarking evaluation for lifelog retrieval systems. Participating systems should have an accurate search engine and a user-friendly interface that can help users to retrieve relevant content. In this paper, we upgrade our previous MyScéal, which was the top performing system in LSC'20 and LSC'21, and present E-MyScéal for LSC'22, which includes a completely different search engine. Instead of using visual concepts for retrieval such as MyScéal, the new E-MyScéal employs an embedding technique that facilitates novice users who are not familiar with the concepts. Our experiments show that the new search engine can find relevant images in the first place in the ranked list, four a quarter of the LSC'21 queries (26\%) by using just the first hint from the textual information need. Regarding the user interface, we still keep the simple non-faceted design as in the previous version but improve the event view browsing in order to better support novice users.},
	urldate = {2022-07-13},
	booktitle = {Proceedings of the 5th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Tran, Ly-Duyen and Nguyen, Manh-Duy and Nguyen, Binh and Lee, Hyowon and Zhou, Liting and Gurrin, Cathal},
	month = jun,
	year = {2022},
	keywords = {interactive retrieval system, lifelog, human factors},
	pages = {32--37},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\7N9BJGPI\\Tran et al. - 2022 - E-Myscéal Embedding-based Interactive Lifelog Ret.pdf:application/pdf},
}

@inproceedings{cheng_rethinking_2021,
	title = {Rethinking {Space}-{Time} {Networks} with {Improved} {Memory} {Coverage} for {Efficient} {Video} {Object} {Segmentation}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/61b4a64be663682e8cb037d9719ad8cd-Abstract.html},
	abstract = {This paper presents a simple yet effective approach to modeling space-time correspondences in the context of video object segmentation. Unlike most existing approaches, we establish correspondences directly between frames without re-encoding the mask features for every object, leading to a highly efficient and robust framework. With the correspondences, every node in the current query frame is inferred by aggregating features from the past in an associative fashion. We cast the aggregation process as a voting problem and find that the existing inner-product affinity leads to poor use of memory with a small (fixed) subset of memory nodes dominating the votes, regardless of the query. In light of this phenomenon, we propose using the negative squared Euclidean distance instead to compute the affinities. We validated that every memory node now has a chance to contribute, and experimentally showed that such diversified voting is beneficial to both memory efficiency and inference accuracy. The synergy of correspondence networks and diversified voting works exceedingly well, achieves new state-of-the-art results on both DAVIS and YouTubeVOS datasets while running significantly faster at 20+ FPS for multiple objects without bells and whistles.},
	urldate = {2022-07-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Cheng, Ho Kei and Tai, Yu-Wing and Tang, Chi-Keung},
	year = {2021},
	pages = {11781--11794},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\4X4PH9Z7\\Cheng et al. - 2021 - Rethinking Space-Time Networks with Improved Memor.pdf:application/pdf},
}

@inproceedings{yang_associating_2021,
	title = {Associating {Objects} with {Transformers} for {Video} {Object} {Segmentation}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/147702db07145348245dc5a2f2fe5683-Abstract.html},
	abstract = {This paper investigates how to realize better and more efficient embedding learning to tackle the semi-supervised video object segmentation under challenging multi-object scenarios. The state-of-the-art methods learn to decode features with a single positive object and thus have to match and segment each target separately under multi-object scenarios, consuming multiple times computing resources. To solve the problem, we propose an Associating Objects with Transformers (AOT) approach to match and decode multiple objects uniformly. In detail, AOT employs an identification mechanism to associate multiple targets into the same high-dimensional embedding space. Thus, we can simultaneously process multiple objects' matching and segmentation decoding as efficiently as processing a single object. For sufficiently modeling multi-object association, a Long Short-Term Transformer is designed for constructing hierarchical matching and propagation. We conduct extensive experiments on both multi-object and single-object benchmarks to examine AOT variant networks with different complexities. Particularly, our R50-AOT-L outperforms all the state-of-the-art competitors on three popular benchmarks, i.e., YouTube-VOS (84.1\% J\&F), DAVIS 2017 (84.9\%), and DAVIS 2016 (91.1\%), while keeping more than 3X faster multi-object run-time. Meanwhile, our AOT-T can maintain real-time multi-object speed on the above benchmarks. Based on AOT, we ranked 1st in the 3rd Large-scale VOS Challenge.},
	urldate = {2022-07-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yang, Zongxin and Wei, Yunchao and Yang, Yi},
	year = {2021},
	pages = {2491--2502},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\46KFKD8S\\Yang et al. - 2021 - Associating Objects with Transformers for Video Ob.pdf:application/pdf},
}

@inproceedings{oh_video_2019,
	title = {Video {Object} {Segmentation} {Using} {Space}-{Time} {Memory} {Networks}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Oh_Video_Object_Segmentation_Using_Space-Time_Memory_Networks_ICCV_2019_paper.html},
	urldate = {2022-07-14},
	author = {Oh, Seoung Wug and Lee, Joon-Young and Xu, Ning and Kim, Seon Joo},
	year = {2019},
	pages = {9226--9235},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\WBYRTU5C\\Oh et al. - 2019 - Video Object Segmentation Using Space-Time Memory .pdf:application/pdf},
}

@inproceedings{heller_vitrivr_2022,
	address = {New York, NY, USA},
	series = {{LSC} '22},
	title = {vitrivr at the {Lifelog} {Search} {Challenge} 2022},
	isbn = {978-1-4503-9239-6},
	url = {https://doi.org/10.1145/3512729.3533003},
	doi = {10.1145/3512729.3533003},
	abstract = {In this paper, we present the iteration of the multimedia retrieval system vitrivr participating at LSC 2022. vitrivr is a general-purpose retrieval system which has previously participated at LSC. We describe the system architecture and functionality, and show initial results based on the test and validation topics.},
	urldate = {2022-07-14},
	booktitle = {Proceedings of the 5th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Heller, Silvan and Rossetto, Luca and Sauter, Loris and Schuldt, Heiko},
	month = jun,
	year = {2022},
	keywords = {lifelogging, lifelog search challenge, content-based retrieval, multimedia retrieval},
	pages = {27--31},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\TIHFL3DL\\Heller et al. - 2022 - vitrivr at the Lifelog Search Challenge 2022.pdf:application/pdf},
}

@inproceedings{cheng_per-pixel_2021,
	title = {Per-{Pixel} {Classification} is {Not} {All} {You} {Need} for {Semantic} {Segmentation}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/950a4152c2b4aa3ad78bdd6b366cc179-Abstract.html},
	abstract = {Modern approaches typically formulate semantic segmentation as a per-pixel classification task, while instance-level segmentation is handled with an alternative mask classification. Our key insight: mask classification is sufficiently general to solve both semantic- and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure. Following this observation, we propose MaskFormer, a simple mask classification model which predicts a set of binary masks, each associated with a single global class label prediction. Overall, the proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results. In particular, we observe that MaskFormer outperforms per-pixel classification baselines when the number of classes is large. Our mask classification-based method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K) and panoptic segmentation (52.7 PQ on COCO) models.},
	urldate = {2022-07-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Cheng, Bowen and Schwing, Alex and Kirillov, Alexander},
	year = {2021},
	pages = {17864--17875},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\94JTZRTX\\Cheng et al. - 2021 - Per-Pixel Classification is Not All You Need for S.pdf:application/pdf},
}

@inproceedings{seo_urvos_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{URVOS}: {Unified} {Referring} {Video} {Object} {Segmentation} {Network} with a {Large}-{Scale} {Benchmark}},
	isbn = {978-3-030-58555-6},
	shorttitle = {{URVOS}},
	doi = {10.1007/978-3-030-58555-6_13},
	abstract = {We propose a unified referring video object segmentation network (URVOS). URVOS takes a video and a referring expression as inputs, and estimates the object masks referred by the given language expression in the whole video frames. Our algorithm addresses the challenging problem by performing language-based object segmentation and mask propagation jointly using a single deep neural network with a proper combination of two attention models. In addition, we construct the first large-scale referring video object segmentation dataset called Refer-Youtube-VOS. We evaluate our model on two benchmark datasets including ours and demonstrate the effectiveness of the proposed approach. The dataset is released at https://github.com/skynbe/Refer-Youtube-VOS.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Seo, Seonguk and Lee, Joon-Young and Han, Bohyung},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Referring object segmentation, Video object segmentation},
	pages = {208--223},
}

@inproceedings{margffoy-tuay_dynamic_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Dynamic {Multimodal} {Instance} {Segmentation} {Guided} by {Natural} {Language} {Queries}},
	isbn = {978-3-030-01252-6},
	doi = {10.1007/978-3-030-01252-6_39},
	abstract = {We address the problem of segmenting an object given a natural language expression that describes it. Current techniques tackle this task by either (i) directly or recursively merging linguistic and visual information in the channel dimension and then performing convolutions; or by (ii) mapping the expression to a space in which it can be thought of as a filter, whose response is directly related to the presence of the object at a given spatial coordinate in the image, so that a convolution can be applied to look for the object. We propose a novel method that integrates these two insights in order to fully exploit the recursive nature of language. Additionally, during the upsampling process, we take advantage of the intermediate information generated when downsampling the image, so that detailed segmentations can be obtained. We compare our method against the state-of-the-art approaches in four standard datasets, in which it surpasses all previous methods in six of eight of the splits for this task.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Margffoy-Tuay, Edgar and Pérez, Juan C. and Botero, Emilio and Arbeláez, Pablo},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Dynamic convolutional filters, Instance segmentation, Multimodal interaction, Natural language processing, Referring expressions},
	pages = {656--672},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\4ZAGAIW2\\Margffoy-Tuay et al. - 2018 - Dynamic Multimodal Instance Segmentation Guided by.pdf:application/pdf},
}

@inproceedings{liu_recurrent_2017,
	title = {Recurrent {Multimodal} {Interaction} for {Referring} {Image} {Segmentation}},
	doi = {10.1109/ICCV.2017.143},
	abstract = {In this paper we are interested in the problem of image segmentation given natural language descriptions, i.e. referring expressions. Existing works tackle this problem by first modeling images and sentences independently and then segment images by combining these two types of representations. We argue that learning word-to-image interaction is more native in the sense of jointly modeling two modalities for the image segmentation task, and we propose convolutional multimodal LSTM to encode the sequential interactions between individual words, visual information, and spatial information. We show that our proposed model outperforms the baseline model on benchmark datasets. In addition, we analyze the intermediate output of the proposed multimodal LSTM approach and empirically explain how this approach enforces a more effective word-to-image interaction.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Liu, Chenxi and Lin, Zhe and Shen, Xiaohui and Yang, Jimei and Lu, Xin and Yuille, Alan},
	month = oct,
	year = {2017},
	note = {ISSN: 2380-7504},
	keywords = {Convolution, Feature extraction, Image segmentation, Natural languages, Semantics, Visualization},
	pages = {1280--1289},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\JQCR37HV\\Liu et al. - 2017 - Recurrent Multimodal Interaction for Referring Ima.pdf:application/pdf},
}

@inproceedings{li_referring_2018,
	title = {Referring {Image} {Segmentation} via {Recurrent} {Refinement} {Networks}},
	doi = {10.1109/CVPR.2018.00602},
	abstract = {We address the problem of image segmentation from natural language descriptions. Existing deep learning-based methods encode image representations based on the output of the last convolutional layer. One general issue is that the resulting image representation lacks multi-scale semantics, which are key components in advanced segmentation systems. In this paper, we utilize the feature pyramids inherently existing in convolutional neural networks to capture the semantics at different scales. To produce suitable information flow through the path of feature hierarchy, we propose Recurrent Refinement Network (RRN) that takes pyramidal features as input to refine the segmentation mask progressively. Experimental results on four available datasets show that our approach outperforms multiple baselines and state-of-the-art1.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Li, Ruiyu and Li, Kaican and Kuo, Yi-Chun and Shu, Michelle and Qi, Xiaojuan and Shen, Xiaoyong and Jia, Jiaya},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {Feature extraction, Image segmentation, Natural languages, Semantics, Logic gates, Task analysis, Training},
	pages = {5745--5753},
}

@inproceedings{voigtlaender_online_2017,
	address = {London, UK},
	title = {Online {Adaptation} of {Convolutional} {Neural} {Networks} for {Video} {Object} {Segmentation}},
	isbn = {978-1-901725-60-5},
	url = {http://www.bmva.org/bmvc/2017/papers/paper116/index.html},
	doi = {10.5244/C.31.116},
	language = {en},
	urldate = {2022-07-23},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2017},
	publisher = {British Machine Vision Association},
	author = {Voigtlaender, Paul and Leibe, Bastian},
	year = {2017},
	pages = {116},
	file = {Full Text:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\9G83L7FK\\Voigtlaender and Leibe - 2017 - Online Adaptation of Convolutional Neural Networks.pdf:application/pdf},
}

@inproceedings{caelles_one-shot_2017,
	address = {Honolulu, HI},
	title = {One-{Shot} {Video} {Object} {Segmentation}},
	isbn = {978-1-5386-0457-1},
	url = {https://ieeexplore.ieee.org/document/8100048/},
	doi = {10.1109/CVPR.2017.565},
	language = {en},
	urldate = {2022-07-23},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Caelles, S. and Maninis, K.-K. and Pont-Tuset, J. and Leal-Taixe, L. and Cremers, D. and Van Gool, L.},
	month = jul,
	year = {2017},
	pages = {5320--5329},
	file = {Caelles et al. - 2017 - One-Shot Video Object Segmentation.pdf:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\YMK4K8T3\\Caelles et al. - 2017 - One-Shot Video Object Segmentation.pdf:application/pdf},
}

@article{maninis_video_2019,
	title = {Video {Object} {Segmentation} without {Temporal} {Information}},
	volume = {41},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2018.2838670},
	abstract = {Video Object Segmentation, and video processing in general, has been historically dominated by methods that rely on the temporal consistency and redundancy in consecutive video frames. When the temporal smoothness is suddenly broken, such as when an object is occluded, or some frames are missing in a sequence, the result of these methods can deteriorate significantly. This paper explores the orthogonal approach of processing each frame independently, i.e., disregarding the temporal information. In particular, it tackles the task of semi-supervised video object segmentation: the separation of an object from the background in a video, given its mask in the first frame. We present Semantic One-Shot Video Object Segmentation (OSVOSS), based on a fully-convolutional neural network architecture that is able to successively transfer generic semantic information, learned on ImageNet, to the task of foreground segmentation, and finally to learning the appearance of a single annotated object of the test sequence (hence one shot). We show that instance-level semantic information, when combined effectively, can dramatically improve the results of our previous method, OSVOS. We perform experiments on two recent single-object video segmentation databases, which show that OSVOSS is both the fastest and most accurate method in the state of the art. Experiments on multi-object video segmentation show that OSVOSS obtains competitive results.},
	number = {6},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Maninis, K.-K. and Caelles, S. and Chen, Y. and Pont-Tuset, J. and Leal-Taixé, L. and Cremers, D. and Van Gool, L.},
	month = jun,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Video object segmentation, Image segmentation, Semantics, Task analysis, Training, Computer architecture, convolutional neural networks, instance segmentation, Motion segmentation, Object segmentation, semantic segmentation},
	pages = {1515--1530},
	file = {Accepted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\5VXZZQS5\\Maninis et al. - 2019 - Video Object Segmentation without Temporal Informa.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\R3KW86UY\\8362936.html:text/html},
}

@inproceedings{shin_photocube_2021,
	address = {New York, NY, USA},
	series = {{LSC} '21},
	title = {{PhotoCube} at the {Lifelog} {Search} {Challenge} 2021},
	isbn = {978-1-4503-8533-6},
	url = {https://doi.org/10.1145/3463948.3469073},
	doi = {10.1145/3463948.3469073},
	abstract = {The Lifelog Search Challenge (LSC) is a venue where retrieval system researchers compete in solving tasks to retrieve the correct image from a lifelog collection. At LSC 2021, we introduce the PhotoCube system as a new competitor. PhotoCube is an interactive media retrieval system that considers media items to exist in a hypercube in multidimensional metadata space. To solve tasks, users explore the contents of the hypercube by dynamically (a) applying a variety of filters and (b) projecting the hypercube to a three-dimensional cube that is visualised on screen.},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 4th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Shin, Jihye and Waldau, Alexandra and Duane, Aaron and Jónsson, Björn Þór},
	month = aug,
	year = {2021},
	keywords = {lifelogging, interactive multimedia retrieval, M3},
	pages = {59--63},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	doi = {10.1109/CVPR.2016.90},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2016, {Las} {Vegas}, {NV}, {USA}, {June} 27-30, 2016},
	publisher = {IEEE Computer Society},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\BBEEKSHW\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@article{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	volume = {abs/1907.11692},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	urldate = {2022-05-04},
	journal = {CoRR},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	year = {2019},
	note = {arXiv: 1907.11692},
}

@inproceedings{trang-trung_flexible_2021,
	address = {New York, NY, USA},
	series = {{LSC} '21},
	title = {Flexible {Interactive} {Retrieval} {SysTem} 2.0 for {Visual} {Lifelog} {Exploration} at {LSC} 2021},
	isbn = {978-1-4503-8533-6},
	url = {https://doi.org/10.1145/3463948.3469072},
	doi = {10.1145/3463948.3469072},
	abstract = {With a huge collection of photos and video clips, it is essential to provide an efficient and easy-to-use system for users to retrieve moments of interest with a wide variation of query types. This motivates us to develop and upgrade our flexible interactive retrieval system for visual lifelog exploration. In this paper, we briefly introduce version 2 of our system with the following main features. Our system supports multiple modalities for interaction and query processing, including visual query by meta-data, text query and visual information matching based on a joint embedding model, scene clustering based on visual and location information, flexible temporal event navigation, and query expansion with visual examples. With the flexibility in system architecture, we expect our system can easily integrate new modules to enhance its functionalities.},
	booktitle = {Proceedings of the 4th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Trang-Trung, Hoang-Phuc and Le, Thanh-Cong and Tran, Mai-Khiem and Ninh, Van-Tu and Le, Tu-Khiem and Gurrin, Cathal and Tran, Minh-Triet},
	year = {2021},
	note = {event-place: Taipei, Taiwan},
	keywords = {lifelog, information system, interactive retrieval, component integration, joint embedding model},
	pages = {81--87},
}

@incollection{tran_first_2020,
	address = {New York, NY, USA},
	title = {{FIRST} - {Flexible} {Interactive} {Retrieval} {SysTem} for {Visual} {Lifelog} {Exploration} at {LSC} 2020},
	isbn = {978-1-4503-7136-0},
	url = {https://doi.org/10.1145/3379172.3391726},
	abstract = {Lifelog can provide useful insights of our daily activities. It is essential to provide a flexible way for users to retrieve certain events or moments of interest, corresponding to a wide variation of query types. This motivates us to develop FIRST, a Flexible Interactive Retrieval SysTem, to help users to combine or integrate various query components in a flexible manner to handle different query scenarios, such as visual clustering data based on color histogram, visual similarity, GPS location, or scene attributes. We also employ personalized concept detection and image captioning to enhance image understanding from visual lifelog data, and develop an autoencoder-like approach for query text and image feature mapping. Furthermore, we refine the user interface of the retrieval system to better assist users in query expansion and verifying sequential events in a flexible temporal resolution to control the navigation speed through sequences of images.},
	booktitle = {Proceedings of the {Third} {Annual} {Workshop} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Tran, Minh-Triet and Nguyen, Thanh-An and Tran, Quoc-Cuong and Tran, Mai-Khiem and Nguyen, Khanh and Ninh, Van-Tu and Le, Tu-Khiem and Trang-Trung, Hoang-Phuc and Le, Hoang-Anh and Nguyen, Hai-Dang and Do, Trong-Le and Vo-Ho, Viet-Khoa and Gurrin, Cathal},
	year = {2020},
	pages = {67--72},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	urldate = {2022-05-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2017, {December} 4-9, 2017, {Long} {Beach}, {CA}, {USA}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	pages = {5998--6008},
	file = {Full Text:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\8B5DLMSP\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@inproceedings{trang-trung_lifelog_2020,
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Lifelog {Moment} {Retrieval} with {Self}-{Attention} based {Joint} {Embedding} {Model}},
	volume = {2696},
	url = {http://ceur-ws.org/Vol-2696/paper\_60.pdf},
	booktitle = {Working {Notes} of {CLEF} 2020 - {Conference} and {Labs} of the {Evaluation} {Forum}, {Thessaloniki}, {Greece}, {September} 22-25, 2020},
	publisher = {CEUR-WS.org},
	author = {Trang-Trung, Hoang-Phuc and Le, Hoang-Anh and Tran, Minh-Triet},
	editor = {Cappellato, Linda and Eickhoff, Carsten and Ferro, Nicola and Névéol, Aurélie},
	year = {2020},
}

@inproceedings{duane_virma_2021,
	address = {New York, NY, USA},
	series = {{LSC} '21},
	title = {{ViRMA}: {Virtual} {Reality} {Multimedia} {Analytics} at {LSC} 2021},
	isbn = {978-1-4503-8533-6},
	shorttitle = {{ViRMA}},
	url = {https://doi.org/10.1145/3463948.3469067},
	doi = {10.1145/3463948.3469067},
	abstract = {In this paper we describe the first iteration of the ViRMA prototype system, a novel approach to multimedia analysis in virtual reality and inspired by the M3 data model. We intend to evaluate our approach via the Lifelog Search Challenge (LSC) to serve as a benchmark against other multimedia analytics systems.},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 4th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Duane, Aaron and Jónsson, Bjorn Þór},
	month = aug,
	year = {2021},
	keywords = {lifelogging, virtual reality, human-computer interaction, multimedia analytics},
	pages = {29--34},
}

@inproceedings{spiess_multimodal_2022,
	address = {New York, NY, USA},
	series = {{LSC} '22},
	title = {Multimodal {Interactive} {Lifelog} {Retrieval} with vitrivr-{VR}},
	isbn = {978-1-4503-9239-6},
	url = {https://doi.org/10.1145/3512729.3533008},
	doi = {10.1145/3512729.3533008},
	abstract = {The multimodal nature of lifelog data poses unique challenges for analysis, indexing and interactive retrieval. To address these challenges, the Lifelog Search Challenge (LSC) is an annual evaluation campaign allowing interactive retrieval systems to explore new ideas and measure their performance against each other. This paper describes the virtual reality (VR) multimedia retrieval system vitrivr-VR, with a focus on aspects relevant to the LSC'22, especially the user interaction in VR, the formulation of typical LSC queries, and different options to explore the retrieval results in VR.},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 5th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Spiess, Florian and Schuldt, Heiko},
	month = jun,
	year = {2022},
	keywords = {interactive lifelog retrieval, lifelog search challenge, virtual reality},
	pages = {38--42},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\UPHMGAEA\\Spiess and Schuldt - 2022 - Multimodal Interactive Lifelog Retrieval with vitr.pdf:application/pdf},
}

@inproceedings{ang_lifeconcept_2021,
	address = {New York, NY, USA},
	series = {{LSC} '21},
	title = {{LifeConcept}: {An} {Interactive} {Approach} for {Multimodal} {Lifelog} {Retrieval} through {Concept} {Recommendation}},
	isbn = {978-1-4503-8533-6},
	shorttitle = {{LifeConcept}},
	url = {https://doi.org/10.1145/3463948.3469070},
	doi = {10.1145/3463948.3469070},
	abstract = {The major challenge in visual lifelog retrieval is the semantic gap between textual queries and visual concepts. This paper presents our work on the Lifelog Search Challenge 2021 (LSC'21), an annual comparative benchmarking activity for comparing approaches to interactive retrieval from multimodal lifelogs. We propose LifeConcept, an interactive lifelog search system that is aimed at accelerating the retrieval process and retrieving more precise results. In this work, we introduce several new features such as the number of people, location cluster, and object with color. Moreover, we obtain visual concepts from the images with computer vision models and propose a concept recommendation method to reduce the semantic gap. In this way, users can efficiently set up the related conditions for their requirements and search the desired images with appropriate query terms based on the suggestion.},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 4th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Ang, Wei-Hong and Yen, An-Zi and Chu, Tai-Te and Huang, Hen-Hsen and Chen, Hsin-Hsi},
	month = aug,
	year = {2021},
	keywords = {image retrieval, interactive system, multimodal retrieval, visual lifelogging},
	pages = {47--51},
}

@inproceedings{lokoc_enhanced_2021,
	address = {New York, NY, USA},
	series = {{LSC} '21},
	title = {Enhanced {SOMHunter} for {Known}-item {Search} in {Lifelog} {Data}},
	isbn = {978-1-4503-8533-6},
	url = {https://doi.org/10.1145/3463948.3469074},
	doi = {10.1145/3463948.3469074},
	abstract = {SOMHunter represents a modern light-weight framework for known-item search in datasets of visual data like images or videos. The framework combines an effective W2VV++ text-to-image search approach, a traditional Bayesian like model for maintenance of relevance scores influenced by positive examples, and several types of exploration and exploitation displays. With this initial setting in 2020, already the first prototype of the system turned out to be highly competitive in comparison with other state-of-the-art systems at Video Browser Showdown and Lifelog Search Challenge competitions. In this paper, we present a new version of the system further extending the list of visual data search capabilities. The new version combines localized text queries with collage queries tested at VBS 2021 in two separate systems by our team. Furthermore, the new version of SOMHunter will integrate also the new CLIP text search model recently released by OpenAI. We believe that all the extensions will improve chances to effectively initialize the search that can continue with already supported browsing capabilities.},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 4th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Lokoč, Jakub and Mejzlik, František and Veselý, Patrik and Souček, Tomáš},
	month = aug,
	year = {2021},
	keywords = {deep learning, self-organizing maps, user feedback, video retrieval},
	pages = {71--73},
}

@inproceedings{li_w2vv_2019,
	address = {New York, NY, USA},
	series = {{MM} '19},
	title = {{W2VV}++: {Fully} {Deep} {Learning} for {Ad}-hoc {Video} {Search}},
	isbn = {978-1-4503-6889-6},
	shorttitle = {{W2VV}++},
	url = {https://doi.org/10.1145/3343031.3350906},
	doi = {10.1145/3343031.3350906},
	abstract = {Ad-hoc video search (AVS) is an important yet challenging problem in multimedia retrieval. Different from previous concept-based methods, we propose a fully deep learning method for query representation learning. The proposed method requires no explicit concept modeling, matching and selection. The backbone of our method is the proposed W2VV++ model, a super version of Word2VisualVec (W2VV) previously developed for visual-to-text matching. W2VV++ is obtained by tweaking W2VV with a better sentence encoding strategy and an improved triplet ranking loss. With these simple yet important changes, W2VV++ brings in a substantial improvement. As our participation in the TRECVID 2018 AVS task and retrospective experiments on the TRECVID 2016 and 2017 data show, our best single model, with an overall inferred average precision (infAP) of 0.157, outperforms the state-of-the-art. The performance can be further boosted by model ensemble using late average fusion, reaching a higher infAP of 0.163. With W2VV++, we establish a new baseline for ad-hoc video search.},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Li, Xirong and Xu, Chaoxi and Yang, Gang and Chen, Zhineng and Dong, Jianfeng},
	month = oct,
	year = {2019},
	keywords = {deep learning, ad-hoc video search, cross-modal matching, query representation learning, trecvid benchmarks},
	pages = {1786--1794},
}

@inproceedings{ding_vision-language_2021,
	title = {Vision-{Language} {Transformer} and {Query} {Generation} for {Referring} {Segmentation}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Ding_Vision-Language_Transformer_and_Query_Generation_for_Referring_Segmentation_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-08-02},
	author = {Ding, Henghui and Liu, Chang and Wang, Suchen and Jiang, Xudong},
	year = {2021},
	pages = {16321--16330},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\MEMI4QTQ\\Ding et al. - 2021 - Vision-Language Transformer and Query Generation f.pdf:application/pdf;Snapshot:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\F4WRF64L\\Ding_Vision-Language_Transformer_and_Query_Generation_for_Referring_Segmentation_ICCV_2021_pape.html:text/html},
}

@inproceedings{nguyen_visual-language_2022,
	title = {Visual-{Language} {Transformer} for {Referring} {Video} {Object} {Segmentation}},
	abstract = {Referring Video Object Segmentation task (R-VOS) aims to segment the target object in all video frames referred with a language expression. In this work, we present a Visual-Language Transformer (VLFormer), a query-based network to tackle the R-VOS task. Its key component is the visual-language transformer block (VLB), which associates visual and linguistic features with the object queries effectively and simultaneously. We use the object queries as a set of candidate objects, the Transformer decoder with VLB blocks that guide and interact with candidate objects to find the referred target object. The object tracking is achieved automatically by linking the corresponding queries across frames. Afterwards, a post-processing technique is used to refine and re-track the mask prediction among all the frames. Our model ranks 6th place on Track 3: Referring Video Object Segmentation of the CVPR 2022 Youtube-VOS Challenge.},
	language = {en},
	author = {Nguyen, E-Ro and Hoang-Xuan, Nhat and Tran, Minh-Triet},
	month = {June},
	year = {2022},
	booktitle = {The 4th Large-scale Video Object Segmentation Challenge, The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
	title     = {},
	file = {Nguyen et al. - Visual-Language Transformer for Referring Video Ob.pdf:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\IFCMRPUV\\Nguyen et al. - Visual-Language Transformer for Referring Video Ob.pdf:application/pdf},
}

@inproceedings{tran_v-first_2022,
	address = {Berlin, Heidelberg},
	title = {V-{FIRST}: {A} {Flexible} {Interactive} {Retrieval} {System} for {Video} at {VBS} 2022},
	isbn = {978-3-030-98354-3},
	shorttitle = {V-{FIRST}},
	url = {https://doi.org/10.1007/978-3-030-98355-0_55},
	doi = {10.1007/978-3-030-98355-0_55},
	abstract = {Video retrieval systems have a wide range of applications across multiple domains, therefore the development of user-friendly and efficient systems is necessary. For VBS 2022, we develop a flexible interactive system for video retrieval, namely V-FIRST, that supports two scenarios of usage: query with text descriptions and query with visual examples. We take advantage of both visual and temporal information from videos to extract concepts related to entities, events, scenes, activities, and motion trajectories for video indexing. Our system supports queries with keywords and sentence descriptions as V-FIRST can evaluate the semantic similarities between visual and textual embedding vectors. V-FIRST also allows users to express queries with visual impressions, such as sketches and 2D spatial maps of dominant colors. We use query expansion, elastic temporal video navigation, and intellisense for hints to further boost the performance of our system.},
	urldate = {2022-08-01},
	booktitle = {{MultiMedia} {Modeling}: 28th {International} {Conference}, {MMM} 2022, {Phu} {Quoc}, {Vietnam}, {June} 6–10, 2022, {Proceedings}, {Part} {II}},
	publisher = {Springer-Verlag},
	author = {Tran, Minh-Triet and Hoang-Xuan, Nhat and Trang-Trung, Hoang-Phuc and Le, Thanh-Cong and Tran, Mai-Khiem and Le, Minh-Quan and Le, Tu-Khiem and Ninh, Van-Tu and Gurrin, Cathal},
	month = jun,
	year = {2022},
	keywords = {Color histogram matching, Human-object interaction, Image and video captioning, Interactive system, Moving entity trajectory, Sketch retrieval, Video retrieval},
	pages = {562--568},
}

@inproceedings{wang_end--end_2021,
	title = {End-to-{End} {Video} {Instance} {Segmentation} with {Transformers}},
	doi = {10.1109/CVPR46437.2021.00863},
	abstract = {Video instance segmentation (VIS) is the task that requires simultaneously classifying, segmenting and tracking object instances of interest in video. Recent methods typically develop sophisticated pipelines to tackle this task. Here, we propose a new video instance segmentation framework built upon Transformers, termed VisTR, which views the VIS task as a direct end-to-end parallel sequence decoding/prediction problem. Given a video clip consisting of multiple image frames as input, VisTR outputs the sequence of masks for each instance in the video in order directly. At the core is a new, effective instance sequence matching and segmentation strategy, which supervises and segments instances at the sequence level as a whole. VisTR frames the instance segmentation and tracking in the same perspective of similarity learning, thus considerably simplifying the overall pipeline and is significantly different from existing approaches.Without bells and whistles, VisTR achieves the highest speed among all existing VIS models, and achieves the best result among methods using single model on the YouTube-VIS dataset. For the first time, we demonstrate a much simpler and faster video instance segmentation framework built upon Transformers, achieving competitive accuracy. We hope that VisTR can motivate future research for more video understanding tasks.Code is available at: https://git.io/VisTR},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Wang, Yuqing and Xu, Zhaoliang and Wang, Xinlong and Shen, Chunhua and Cheng, Baoshan and Shen, Hao and Xia, Huaxia},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	keywords = {Image segmentation, Computational modeling, Computer vision, Transformers, Pattern recognition, Pipelines, Transformer cores},
	pages = {8737--8746},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\UR7KXPM7\\Wang et al. - 2021 - End-to-End Video Instance Segmentation with Transf.pdf:application/pdf},
}

@inproceedings{carion_end--end_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	isbn = {978-3-030-58452-8},
	doi = {10.1007/978-3-030-58452-8_13},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {213--229},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\VCIDXRLR\\Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf:application/pdf},
}

@inproceedings{portillo-quintero_straightforward_2021,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Straightforward} {Framework} for {Video} {Retrieval} {Using} {CLIP}},
	volume = {12725},
	url = {https://doi.org/10.1007/978-3-030-77004-4\_1},
	doi = {10.1007/978-3-030-77004-4_1},
	urldate = {2022-08-02},
	booktitle = {Pattern {Recognition} - 13th {Mexican} {Conference}, {MCPR} 2021, {Mexico} {City}, {Mexico}, {June} 23-26, 2021, {Proceedings}},
	publisher = {Springer},
	author = {Portillo-Quintero, Jesús Andrés and Ortiz-Bayliss, José Carlos and Terashima-Marín, Hugo},
	editor = {Roman-Rangel, Edgar and Morales, Ángel Fernando Kuri and Trinidad, José Francisco Martínez and Carrasco-Ochoa, Jesús Ariel and Olvera-López, José Arturo},
	year = {2021},
	pages = {3--12},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\5SMAH6Q6\\Portillo-Quintero et al. - 2021 - A Straightforward Framework for Video Retrieval Us.pdf:application/pdf},
}
