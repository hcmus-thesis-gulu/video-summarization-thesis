\section{Evaluation methods}
\label{section:exp-evaluation}

The evaluation process of our experiments is two-fold with the first part being the automatic evaluation of raw performance between the proposed method and baselines presented in previous works while the second part is the evaluation of the proposed method against human's perspective of the summarization's quality. For the first part of the evaluation procedure, we employ the metric of F-measure which is widely adopted among prior evaluations to measure and compare the performance of our proposal with selected baselines. For the second part, we specifically construct a dedicated survey to collect human's feedback on the quality of the generated summaries in the form of . The following subsections will describe the details of the evaluation process in which the subsection \ref{subsec:exp-evaluation-automatic} provides information about the automatic evaluation process and the \ref{subsec:exp-evaluation-human} contains details for human-centric evaluation.

\subsection{Automatic evaluation}
\label{subsec:exp-evaluation-automatic}
    The automatic evaluation process compares the performance of the method that we proposed in \ref{section:method-model} with the baselines selected among previous works that were mentioned in \ref{chapter:related}. The datasets used for this comparison are already described in previous section \ref{section:exp-datasets}, hence this section is dedicated to the description of the evaluation metric and the baselines.

    \subsubsection{F-measure}
    \label{subsubsec:exp-evaluation-automatic-fmeasure}
        Prior evaluations have adopted F-measure as the main comparison metric for evaluating performance of several video summarization approaches. We would like to revisit the definition of F-measure in this section to provide a better understanding of the metric as well as its nature.

        \paragraph[long]{F-measure nature}
            Given a data consists of several samples and a binary label for each of them, the F-measure metric evaluates a set of predictions on the labels of those samples. To compute the value of the F-measure for a prediction, each of the sample in the dataset is assigned to one of the four categories: true positive (TP), false positive (FP), true negative (TN), and false negative (FN). The assignment is done by evaluating the value of binary label and the prediction with the following rules:
            
            \begin{itemize}
                \item If the label and the prediction are both positive, the sample is assigned to TP.
                \item If the label is positive but the prediction is negative, the sample is assigned to FN.
                \item If the label is negative but the prediction is positive, the sample is assigned to FP.
                \item If both the label and the prediction are negative, the sample is assigned to TN.
            \end{itemize}

            Afterward, the precision value $P$ is computed as the ratio of the number of true positives to the sum of true positives and false positives, meaning that $P = \frac{|TP|}{|TP| + |FP|}$. The recall value $R$ is computed as the ratio of the number of true positives to the sum of true positives and false negatives, or $R = \frac{|TP|}{|TP| + |FN|}$. The F-measure is then defined as the harmonic mean of precision and recall with the formula as follows:

            \begin{equation}
                \label{eq:f-measure}
                F = 2 \times \frac{P \times R}{P + R}
            \end{equation}

            The combination of precision and recall in the evaluation of F-measure is to ensure that the metric is able to capture the performance of the prediction in both positive and negative cases. 
        
        \paragraph[long]{F-measure in video summarization}
            In the case of video summarization, a value of F-measure is calculated for each of the video in the dataset. The prediction is the summary generated by the method being evaluated while the label is the ground truth summary of the video.
            
            Depending on the type of annotations used by the dataset, the groundtruth summaries created by users may be of several different forms that are mentioned below:

            \begin{itemize}
                \item Sets of keyframes: Video's frames are used as main pieces of information to be selected . The ground truth summary of a user is a set of keyframes that are selected by users as the most important frames in the video, meaning that the grountruth $U_i$ of the $i$-th user is a set of keyframes $U_i = \{U_{i}^{(1)}, U_{i}^{(2)}, ..., U_{i}^{(k)}\}$ where $U_{i}^{(j)}$ is the index of the $j$-th keyframe in the video sequence selected by the $i$-th user, and $k$ is the number of keyframes selected by the user.
                \item Sets of key-fragments: The video sequence is partitioned into multiple non-overlapping fragments containing meaningful information in each of them with the method of partitioning depends on the nature of the dataset and is usually proposed along with dataset's publications by its authors. The ground truth summary of a user is a set of key-fragments that are selected by users as the most important fragments in the video, meaning that the grountruth $U_i$ of the $i$-th user is a set of key-fragments $U_i = \{U_{i}^{(1)}, U_{i}^{(2)}, ..., U_{i}^{(k)}\}$ where $U_{i}^{(j)}$ is the index of the $j$-th key-fragment in the video sequence selected by the $i$-th user, and $k$ is the number of key-fragments selected by the user.
                \item Fragment-level scores: Similar to the form of key-fragments, the video sequence is partitioned into multiple non-overlapping fragments containing meaningful information in each of them. The ground truth summary of a user is a set of scores that are assigned to each of the fragments in the video, meaning that the grountruth $U_i$ of the $i$-th user is a set of scores $U_i = \{U_{i}^{(1)}, U_{i}^{(2)}, ..., U_{i}^{(k)}\}$ where $U_{i}^{(j)}$ is the score assigned to the $j$-th fragment in the video sequence by the $i$-th user, and $k$ is the number of fragments in the video.
            \end{itemize}

            To evaluate a method using F-measure on a dataset with one of the above forms of ground truth summary, the method's generated summaries are usually converted to the same form as the ground truth summary. For example, if the ground truth summary is a set of keyframes, the generated one is converted to a set of keyframes as well. The conversion process is usually done by selecting the most important frames or fragments in the video according to the scores assigned to them by the method. Therefore, the pre-evaluation summary of a method for a video is usually of the form $G = \{G^{(1)}, G^{(2)}, ..., G^{(k)}\}$ where $G^{(j)}$ is the index of the $j$-th frame or fragment in the video sequence selected by the method as key information, and $k$ is the number of frames or fragments selected by the method.

            With this set theoretic formulation of generated summary and user's summary, one can define the four categories in preparation of F-measure calculation as follows:

            \begin{itemize}
                \item True positives: Frames or fragments that are selected by both the method and the $i$-th user, or $TP_i = G \cap U_i$.
                \item False positives: Frames or fragments that are selected by the method but not by the $i$-th user, or $FP_i = G \setminus U_i$.
                \item False negatives: Frames or fragments that are selected by the $i$-th user but not by the method, or $FN_i = U_i \setminus G$.
                \item True negatives: Frames or fragments that are not selected by both the method and the $i$-th user, or $TN_i = \{1, 2, ..., n\} \setminus (G \cup U_i)$ where $n$ is the number of frames or fragments in the video.
            \end{itemize}

            From the above formulation, the F-measure of the method for the $i$-th user is calculated using the formula \ref{eq:f-measure} with $P_i = \frac{|TP_i|}{|TP_i| + |FP_i|}$ and $R_i = \frac{|TP_i|}{|TP_i| + |FN_i|}$, leading to $F_i = 2 \times \frac{P_i \times R_i}{P_i + R_i}$. The F-measure of the method for the video is then calculated as the average of F-measure values for all users in the dataset, or $F = \frac{1}{u} \sum_{i=1}^{u} F_i$ where $u$ is the number of users in the dataset.
            
            % The ground truth summary is a binary vector with the length equal to the number of frames in the video. Each element in the vector represents the importance of the corresponding frame in the video. The importance of a frame is determined by the ground truth summary of the video. If the frame is included in the ground truth summary, the corresponding element in the vector is set to 1, otherwise it is set to 0. The prediction is also a binary vector with the same length as the ground truth summary. The importance of a frame in the prediction is determined by the method being evaluated. If the frame is included in the summary generated by the method, the corresponding element in the vector is set to 1, otherwise it is set to 0. The F-measure value of the video is then calculated using the formula \ref{eq:f-measure}.

    \paragraph[long]{Baselines}

\subsection{Human-centric evaluation}
\label{subsec:exp-evaluation-human}