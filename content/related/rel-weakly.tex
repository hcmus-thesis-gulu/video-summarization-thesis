\section{Weakly Supervised approaches}
\label{section:rel-weakly}

Similar to unsupervised approaches, weakly-supervised methods aim to reduce the reliance on extensive sets of hand-labeled data. Instead, they utilize less expensive weak labels that are known to be imperfect compared to full human annotations. However, even with these imperfect labels, weakly-supervised approaches can still create robust predictive models for video summarization.

% Weakly-supervised video summarization methods aim to reduce the reliance on labor-intensive human-generated ground-truth data, similar to unsupervised learning approaches. Instead of completely forgoing ground-truth data, these methods leverage less costly weak labels, such as video-level metadata or sparse annotations for a subset of frames. The underlying hypothesis is that while these labels are imperfect compared to comprehensive human annotations, they can still facilitate the training of effective summarization models.

% This class of methods does not follow a typical analysis pipeline, as they diverge in their approach to learning the summarization task. One of the early approaches in this domain was introduced by Panda et al. (2018) [109]. Their method utilizes video-level metadata to categorize videos and extracts 3D-CNN features to learn a parametric model for categorizing unseen videos. The model is then employed to select video segments that maximize the relevance between the summary and the video category. Panda et al. addressed challenges related to limited dataset size by exploring cross-dataset training, incorporating web-crawled videos, and employing data augmentation techniques to increase the training data.

% Cai et al. (2018) [43] extended the idea of learning summarization from semantically-similar videos in a weakly-supervised setting. They proposed an architecture combining a Variational AutoEncoder (VAE) to learn latent semantics from web videos and a sequence encoder-decoder with attention mechanism for summarization. The VAE's decoding part reconstructs input videos using samples from the learned latent semantics, while the attention mechanism of the encoder-decoder network identifies the most important video fragments. The attention vectors are obtained by integrating the learned latent semantics from collected web videos. The architecture is trained using a weakly-supervised semantic matching loss to learn topic-associated summaries.

% Ho et al. (2018) [110] presented a deep learning framework for summarizing first-person videos but are included here as their method was also evaluated on a dataset used for assessing generic video summarization methods. Recognizing the difficulty of collecting a large amount of fully-annotated first-person video data, they utilized transfer learning principles. Annotated third-person videos, which are more readily available, were used to train the model on how to summarize first-person videos. The algorithm employed cross-domain feature embedding and transfer learning for domain adaptation between third- and first-person videos in a semi-supervised manner.

% Chen et al. (2019) [44] employed the principles of reinforcement learning to construct and train a summarization method using a limited set of human annotations and handcrafted rewards. The rewards encompassed similarity between machine- and human-selected fragments and specific characteristics of the generated summary, such as representativeness. Their method employed a hierarchical key-fragment selection process divided into sub-tasks. Each task was learned through sparse reinforcement learning, utilizing annotations only for a subset of frames rather than exhaustive annotations for the entire set. The final summary was formed based on rewards related to diversity and representativeness.

% These weakly-supervised approaches demonstrate innovative strategies to overcome the limitations of fully-supervised learning while leveraging available weak labels and tailored reward functions to train effective video summarization models.