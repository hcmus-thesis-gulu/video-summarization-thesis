\subsection{Building Object-Oriented Summaries through Key Object Motion}
\label{subsec:rel-unsup-object-oriented}

% Zhang et al. (2018) [108] devised a novel method that prioritizes the retention of fine-grained semantic and motion details within the video summary. Their approach involves an initial preprocessing step aimed at identifying significant objects and their key motions. Leveraging this information, the method represents the entire video by creating segmented object motion clips.

% Subsequently, these clips are fed into the Summarizer, which employs an online motion auto-encoder model known as Stacked Sparse LSTM Auto-Encoder. This model continually updates a customized recurrent auto-encoder network to encode and memorize previous states of object motions. The network's primary task is to reconstruct object-level motion clips, with the reconstruction loss computed between the input and output frames serving as a guide for training the Summarizer.

% Through this training process, the Summarizer becomes proficient in generating summaries that highlight the representative objects in the video and the key motions associated with each object.