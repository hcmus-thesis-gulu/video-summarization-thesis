
@inproceedings{vondrick_tracking_2018,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Tracking {Emerges} by {Colorizing} {Videos}},
	volume = {11217},
	url = {https://doi.org/10.1007/978-3-030-01261-8\_24},
	doi = {10.1007/978-3-030-01261-8_24},
	booktitle = {Computer {Vision} - {ECCV} 2018 - 15th {European} {Conference}, {Munich}, {Germany}, {September} 8-14, 2018, {Proceedings}, {Part} {XIII}},
	publisher = {Springer},
	author = {Vondrick, Carl and Shrivastava, Abhinav and Fathi, Alireza and Guadarrama, Sergio and Murphy, Kevin},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	pages = {402--419},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\ZT4RASLQ\\Vondrick et al. - 2018 - Tracking Emerges by Colorizing Videos.pdf:application/pdf},
}

@inproceedings{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	url = {https://doi.org/10.18653/v1/n19-1423},
	doi = {10.18653/v1/n19-1423},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL}-{HLT} 2019, {Minneapolis}, {MN}, {USA}, {June} 2-7, 2019, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	year = {2019},
	pages = {4171--4186},
}

@article{dosovitskiy_discriminative_2016,
	title = {Discriminative {Unsupervised} {Feature} {Learning} with {Exemplar} {Convolutional} {Neural} {Networks}},
	volume = {38},
	url = {https://doi.org/10.1109/TPAMI.2015.2496141},
	doi = {10.1109/TPAMI.2015.2496141},
	number = {9},
	journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	author = {Dosovitskiy, Alexey and Fischer, Philipp and Springenberg, Jost Tobias and Riedmiller, Martin A. and Brox, Thomas},
	year = {2016},
	pages = {1734--1747},
}

@article{oord_representation_2018,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	volume = {abs/1807.03748},
	url = {http://arxiv.org/abs/1807.03748},
	journal = {CoRR},
	author = {Oord, Aäron van den and Li, Yazhe and Vinyals, Oriol},
	year = {2018},
	note = {arXiv: 1807.03748},
	file = {Full Text:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\FK6GL3CI\\Oord et al. - 2018 - Representation Learning with Contrastive Predictiv.pdf:application/pdf},
}

@inproceedings{he_momentum_2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	url = {https://doi.org/10.1109/CVPR42600.2020.00975},
	doi = {10.1109/CVPR42600.2020.00975},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2020, {Seattle}, {WA}, {USA}, {June} 13-19, 2020},
	publisher = {Computer Vision Foundation / IEEE},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross B.},
	year = {2020},
	pages = {9726--9735},
}

@inproceedings{radford_learning_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	volume = {139},
	url = {http://proceedings.mlr.press/v139/radford21a.html},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}, {ICML} 2021, 18-24 {July} 2021, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	editor = {Meila, Marina and Zhang, Tong},
	year = {2021},
	pages = {8748--8763},
	file = {Full Text:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\AXJGQKW7\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf},
}

@inproceedings{jia_scaling_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Scaling {Up} {Visual} and {Vision}-{Language} {Representation} {Learning} {With} {Noisy} {Text} {Supervision}},
	volume = {139},
	url = {http://proceedings.mlr.press/v139/jia21b.html},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}, {ICML} 2021, 18-24 {July} 2021, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V. and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
	editor = {Meila, Marina and Zhang, Tong},
	year = {2021},
	pages = {4904--4916},
}

@inproceedings{chen_uniter_2020,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{UNITER}: {UNiversal} {Image}-{TExt} {Representation} {Learning}},
	volume = {12375},
	url = {https://doi.org/10.1007/978-3-030-58577-8\_7},
	doi = {10.1007/978-3-030-58577-8_7},
	booktitle = {Computer {Vision} - {ECCV} 2020 - 16th {European} {Conference}, {Glasgow}, {UK}, {August} 23-28, 2020, {Proceedings}, {Part} {XXX}},
	publisher = {Springer},
	author = {Chen, Yen-Chun and Li, Linjie and Yu, Licheng and Kholy, Ahmed El and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {104--120},
}

@inproceedings{anderson_bottom-up_2018,
	title = {Bottom-{Up} and {Top}-{Down} {Attention} for {Image} {Captioning} and {Visual} {Question} {Answering}},
	booktitle = {The {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE Computer Society},
	author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
	year = {2018},
	keywords = {attention caption image paper-dzhi q\&a},
	pages = {6077--6086},
}

@inproceedings{rao_denseclip_2022,
	title = {{DenseCLIP}: {Language}-{Guided} {Dense} {Prediction} {With} {Context}-{Aware} {Prompting}},
	shorttitle = {{DenseCLIP}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-06},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Rao, Yongming and Zhao, Wenliang and Chen, Guangyi and Tang, Yansong and Zhu, Zheng and Huang, Guan and Zhou, Jie and Lu, Jiwen},
	year = {2022},
	pages = {18082--18091},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\35MMKWLJ\\Rao et al. - 2022 - DenseCLIP Language-Guided Dense Prediction With C.pdf:application/pdf;Snapshot:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\B73UTQRY\\Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.html:text/html},
}

@inproceedings{liang_rethinking_2021,
	title = {Rethinking {Cross}-modal {Interaction} from a {Top}-down {Perspective} for {Referring} {Video} {Object} {Segmentation}},
	abstract = {Referring video object segmentation (RVOS) aims to segment video objects with the guidance of natural language reference. Previous methods typically tackle RVOS through directly grounding linguistic reference over the image lattice. Such bottom-up strategy fails to explore object-level cues, easily leading to inferior results. In this work, we instead put forward a two-stage, top-down RVOS solution. First, an exhaustive set of object tracklets is constructed by propagating object masks detected from several sampled frames to the entire video. Second, a Transformer-based tracklet-language grounding module is proposed, which models instance-level visual relations and cross-modal interactions simultaneously and efﬁciently. Our model ranks 1st place on CVPR2021 Referring Youtube-VOS challenge.},
	language = {en},
	booktitle = {The {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}) {Workshops}, {YouTube}-{VOS}},
	author = {Liang, Chen and Wu, Yu and Zhou, Tianfei and Wang, Wenguan and Yang, Zongxin and Wei, Yunchao and Yang, Yi},
	year = {2021},
	pages = {4},
	file = {Liang et al. - Rethinking Cross-modal Interaction from a Top-down.pdf:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\KJNRIJ7M\\Liang et al. - Rethinking Cross-modal Interaction from a Top-down.pdf:application/pdf},
}

@inproceedings{ding_progressive_2021,
	title = {Progressive {Multimodal} {Interaction} {Network} for {Referring} {Video} {Object} {Segmentation}},
	abstract = {Referring video object segmentation aims to segment the target object in the video referred by a natural language description. Existing methods perform the coarse late multimodal fusion to align visual and linguistic modalities, identifying the referent matched with the description. Then the memory attention among frames is conducted to reﬁne the results in other frames. To achieve ﬁner multimodal feature fusion, we propose a Progressive Multimodal Interaction Network (PMINet) which performs multimodal feature fusion in each stage of visual backbone, enabling the progressive learning of visual features under the guidance of linguistic features. Afterwards, we conduct other postprocessing techniques to reﬁne the mask prediction among all the frames, yielding the temporal consistency of segmentation result of the whole video. Our proposed method achieves the second place on the Track 3: Referring Video Object Segmentation of the 2021 YouTube VOS Challenge.},
	language = {en},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Ding, Zihan and Hui, Tianrui and Huang, Shaofei and Liu, Si and Luo, Xuan and Huang, Junshi and Wei, Xiaoming},
	year = {2021},
	pages = {4},
	file = {Ding et al. - Progressive Multimodal Interaction Network for Ref.pdf:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\3NEU7VPB\\Ding et al. - Progressive Multimodal Interaction Network for Ref.pdf:application/pdf},
}

@inproceedings{wu_language_2022,
	title = {Language {As} {Queries} for {Referring} {Video} {Object} {Segmentation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Wu_Language_As_Queries_for_Referring_Video_Object_Segmentation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-13},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wu, Jiannan and Jiang, Yi and Sun, Peize and Yuan, Zehuan and Luo, Ping},
	year = {2022},
	pages = {4974--4984},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\F6ICFMI7\\Wu et al. - 2022 - Language As Queries for Referring Video Object Seg.pdf:application/pdf},
}

@inproceedings{botach_end--end_2022,
	title = {End-to-{End} {Referring} {Video} {Object} {Segmentation} {With} {Multimodal} {Transformers}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Botach_End-to-End_Referring_Video_Object_Segmentation_With_Multimodal_Transformers_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-13},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Botach, Adam and Zheltonozhskii, Evgenii and Baskin, Chaim},
	year = {2022},
	pages = {4985--4995},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\EGTZPDDA\\Botach et al. - 2022 - End-to-End Referring Video Object Segmentation Wit.pdf:application/pdf},
}

@inproceedings{cheng_masked-attention_2022,
	title = {Masked-{Attention} {Mask} {Transformer} for {Universal} {Image} {Segmentation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Cheng_Masked-Attention_Mask_Transformer_for_Universal_Image_Segmentation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-13},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Cheng, Bowen and Misra, Ishan and Schwing, Alexander G. and Kirillov, Alexander and Girdhar, Rohit},
	year = {2022},
	pages = {1290--1299},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\V7T74JH2\\Cheng et al. - 2022 - Masked-Attention Mask Transformer for Universal Im.pdf:application/pdf;Snapshot:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\DCBHUHDD\\Cheng_Masked-Attention_Mask_Transformer_for_Universal_Image_Segmentation_CVPR_2022_paper.html:text/html},
}

@inproceedings{wang_cris_2022,
	title = {{CRIS}: {CLIP}-{Driven} {Referring} {Image} {Segmentation}},
	shorttitle = {{CRIS}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Wang_CRIS_CLIP-Driven_Referring_Image_Segmentation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-13},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wang, Zhaoqing and Lu, Yu and Li, Qiang and Tao, Xunqiang and Guo, Yandong and Gong, Mingming and Liu, Tongliang},
	year = {2022},
	pages = {11686--11695},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\KZKTY7V4\\Wang et al. - 2022 - CRIS CLIP-Driven Referring Image Segmentation.pdf:application/pdf;Snapshot:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\YCEZXSCL\\Wang_CRIS_CLIP-Driven_Referring_Image_Segmentation_CVPR_2022_paper.html:text/html},
}

@inproceedings{gurrin_introduction_2022,
	address = {New York, NY, USA},
	series = {{ICMR} '22},
	title = {Introduction to the {Fifth} {Annual} {Lifelog} {Search} {Challenge}, {LSC}'22},
	isbn = {978-1-4503-9238-9},
	url = {https://doi.org/10.1145/3512527.3531439},
	doi = {10.1145/3512527.3531439},
	abstract = {For the fifth time since 2018, the Lifelog Search Challenge (LSC) facilitated a benchmarking exercise to compare interactive search systems designed for multimodal lifelogs. LSC'22 attracted nine participating research groups who developed interactive lifelog retrieval systems enabling fast and effective access to lifelogs. The systems competed in front of a hybrid audience at the LSC workshop at ACM ICMR'22. This paper presents an introduction to the LSC workshop, the new (larger) dataset used in the competition, and introduces the participating lifelog search systems.},
	urldate = {2022-07-13},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Gurrin, Cathal and Zhou, Liting and Healy, Graham and Þór Jónsson, Björn and Dang-Nguyen, Duc-Tien and Lokoć, Jakub and Tran, Minh-Triet and Hürst, Wolfgang and Rossetto, Luca and Schöffmann, Klaus},
	month = jun,
	year = {2022},
	keywords = {lifelog, benchmarking, interactive retrieval systems},
	pages = {685--687},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\4G79BDN8\\Gurrin et al. - 2022 - Introduction to the Fifth Annual Lifelog Search Ch.pdf:application/pdf},
}

@inproceedings{alam_memento_2022,
	address = {New York, NY, USA},
	series = {{LSC} '22},
	title = {Memento 2.0: {An} {Improved} {Lifelog} {Search} {Engine} for {LSC}'22},
	isbn = {978-1-4503-9239-6},
	shorttitle = {Memento 2.0},
	url = {https://doi.org/10.1145/3512729.3533006},
	doi = {10.1145/3512729.3533006},
	abstract = {In this paper, we present Memento 2.0, an improved version of our system which first participated in the Lifelog Search Challenge 2021. Memento 2.0 employs image-text embeddings derived from two CLIP models (ViT-L/14 and ResNet-50x64) and adopts a weighted ensemble approach to derive a combined final ranking. Our approach significantly improves the performance over the baseline LSC'21 system. We additionally make important updates to the system's user interface after analysing the shortcomings to make it more efficient and better suited to the needs of the Lifelog Search Challenge.},
	urldate = {2022-07-13},
	booktitle = {Proceedings of the 5th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Alam, Naushad and Graham, Yvette and Gurrin, Cathal},
	month = jun,
	year = {2022},
	keywords = {information systems, retrieval models and ranking, search interfaces},
	pages = {2--7},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\4GW3ZQJP\\Alam et al. - 2022 - Memento 2.0 An Improved Lifelog Search Engine for.pdf:application/pdf},
}

@inproceedings{nguyen_lifeseeker_2022,
	address = {New York, NY, USA},
	series = {{LSC} '22},
	title = {{LifeSeeker} 4.0: {An} {Interactive} {Lifelog} {Search} {Engine} for {LSC}'22},
	isbn = {978-1-4503-9239-6},
	shorttitle = {{LifeSeeker} 4.0},
	url = {https://doi.org/10.1145/3512729.3533014},
	doi = {10.1145/3512729.3533014},
	abstract = {In this paper, we introduce LifeSeeker 4.0 - an interactive lifelog retrieval system developed for the fifth annual Lifelog Search Challenge (LSC'22). In LifeSeeker 4.0, we focus on enhancing our previous system to allow users who have little to no knowledge of underlying system functioning and lifelog data to use it with ease by employing a Contrastive Language-Image Pre-training (CLIP) model. Furthermore, we have exploited the music metadata to facilitate searches that may incorporate emotion. Event clustering is also improved in this version to increase user experience by reducing the occurrence of repeated images, and hence decreasing the search time.},
	urldate = {2022-07-13},
	booktitle = {Proceedings of the 5th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Thao-Nhu and Le, Tu-Khiem and Ninh, Van-Tu and Tran, Minh-Triet and Nguyen, Thanh Binh and Healy, Graham and Smyth, Sinéad and Caputo, Annalina and Gurrin, Cathal},
	month = jun,
	year = {2022},
	keywords = {lifelog, information system, interactive retrieval},
	pages = {14--19},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\Y59NLRBF\\Nguyen et al. - 2022 - LifeSeeker 4.0 An Interactive Lifelog Search Engi.pdf:application/pdf},
}

@inproceedings{hoang-xuan_flexible_2022,
	address = {New York, NY, USA},
	series = {{LSC} '22},
	title = {Flexible {Interactive} {Retrieval} {SysTem} 3.0 for {Visual} {Lifelog} {Exploration} at {LSC} 2022},
	isbn = {978-1-4503-9239-6},
	url = {https://doi.org/10.1145/3512729.3533013},
	doi = {10.1145/3512729.3533013},
	abstract = {Building a retrieval system with lifelogging data is more complicated than with ordinary data due to the redundancies, blurriness, massive amount of data, various sources of information accompanying lifelogging data, and especially the ad-hoc nature of queries. The Lifelog Search Challenge (LSC) is a benchmarking challenge that encourages researchers and developers to push the boundaries in lifelog retrieval. For LSC'22, we develop FIRST 3.0, a novel and flexible system that leverages expressive cross-domain embeddings to enhance the searching process. Our system aims to adaptively capture the semantics of an image at different levels of detail. We also propose to augment our system with an external search engine to help our system with initial visual examples for unfamiliar concepts. Finally, we organize image data in hierarchical clusters based on their visual similarity and location to assist users in data exploration. Experiments show that our system is both fast and effective in handling various retrieval scenarios.},
	urldate = {2022-07-13},
	booktitle = {Proceedings of the 5th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Hoang-Xuan, Nhat and Trang-Trung, Hoang-Phuc and Nguyen, E-Ro and Le, Thanh-Cong and Tran, Mai-Khiem and Le, Tu-Khiem and Ninh, Van-Tu and Gurrin, Cathal and Tran, Minh-Triet},
	month = jun,
	year = {2022},
	keywords = {lifelog, interactive retrieval systems, query expansion, semantic embedding},
	pages = {20--26},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\S8I67VRN\\Hoang-Xuan et al. - 2022 - Flexible Interactive Retrieval SysTem 3.0 for Visu.pdf:application/pdf},
}

@inproceedings{tran_e-mysce_2022,
	address = {New York, NY, USA},
	series = {{LSC} '22},
	title = {E-{Myscéal}: {Embedding}-based {Interactive} {Lifelog} {Retrieval} {System} for {LSC}'22},
	isbn = {978-1-4503-9239-6},
	shorttitle = {E-{Myscéal}},
	url = {https://doi.org/10.1145/3512729.3533012},
	doi = {10.1145/3512729.3533012},
	abstract = {Developing interactive lifelog retrieval systems is a growing research area. There are many international competitions for lifelog retrieval that encourage researchers to build effective systems that can address the multimodal retrieval challenge of lifelogs. The Lifelog Search Challenge (LSC) was first organised in 2018 and is currently the only interactive benchmarking evaluation for lifelog retrieval systems. Participating systems should have an accurate search engine and a user-friendly interface that can help users to retrieve relevant content. In this paper, we upgrade our previous MyScéal, which was the top performing system in LSC'20 and LSC'21, and present E-MyScéal for LSC'22, which includes a completely different search engine. Instead of using visual concepts for retrieval such as MyScéal, the new E-MyScéal employs an embedding technique that facilitates novice users who are not familiar with the concepts. Our experiments show that the new search engine can find relevant images in the first place in the ranked list, four a quarter of the LSC'21 queries (26\%) by using just the first hint from the textual information need. Regarding the user interface, we still keep the simple non-faceted design as in the previous version but improve the event view browsing in order to better support novice users.},
	urldate = {2022-07-13},
	booktitle = {Proceedings of the 5th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Tran, Ly-Duyen and Nguyen, Manh-Duy and Nguyen, Binh and Lee, Hyowon and Zhou, Liting and Gurrin, Cathal},
	month = jun,
	year = {2022},
	keywords = {interactive retrieval system, lifelog, human factors},
	pages = {32--37},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\7N9BJGPI\\Tran et al. - 2022 - E-Myscéal Embedding-based Interactive Lifelog Ret.pdf:application/pdf},
}

@inproceedings{cheng_rethinking_2021,
	title = {Rethinking {Space}-{Time} {Networks} with {Improved} {Memory} {Coverage} for {Efficient} {Video} {Object} {Segmentation}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/61b4a64be663682e8cb037d9719ad8cd-Abstract.html},
	abstract = {This paper presents a simple yet effective approach to modeling space-time correspondences in the context of video object segmentation. Unlike most existing approaches, we establish correspondences directly between frames without re-encoding the mask features for every object, leading to a highly efficient and robust framework. With the correspondences, every node in the current query frame is inferred by aggregating features from the past in an associative fashion. We cast the aggregation process as a voting problem and find that the existing inner-product affinity leads to poor use of memory with a small (fixed) subset of memory nodes dominating the votes, regardless of the query. In light of this phenomenon, we propose using the negative squared Euclidean distance instead to compute the affinities. We validated that every memory node now has a chance to contribute, and experimentally showed that such diversified voting is beneficial to both memory efficiency and inference accuracy. The synergy of correspondence networks and diversified voting works exceedingly well, achieves new state-of-the-art results on both DAVIS and YouTubeVOS datasets while running significantly faster at 20+ FPS for multiple objects without bells and whistles.},
	urldate = {2022-07-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Cheng, Ho Kei and Tai, Yu-Wing and Tang, Chi-Keung},
	year = {2021},
	pages = {11781--11794},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\4X4PH9Z7\\Cheng et al. - 2021 - Rethinking Space-Time Networks with Improved Memor.pdf:application/pdf},
}

@inproceedings{yang_associating_2021,
	title = {Associating {Objects} with {Transformers} for {Video} {Object} {Segmentation}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/147702db07145348245dc5a2f2fe5683-Abstract.html},
	abstract = {This paper investigates how to realize better and more efficient embedding learning to tackle the semi-supervised video object segmentation under challenging multi-object scenarios. The state-of-the-art methods learn to decode features with a single positive object and thus have to match and segment each target separately under multi-object scenarios, consuming multiple times computing resources. To solve the problem, we propose an Associating Objects with Transformers (AOT) approach to match and decode multiple objects uniformly. In detail, AOT employs an identification mechanism to associate multiple targets into the same high-dimensional embedding space. Thus, we can simultaneously process multiple objects' matching and segmentation decoding as efficiently as processing a single object. For sufficiently modeling multi-object association, a Long Short-Term Transformer is designed for constructing hierarchical matching and propagation. We conduct extensive experiments on both multi-object and single-object benchmarks to examine AOT variant networks with different complexities. Particularly, our R50-AOT-L outperforms all the state-of-the-art competitors on three popular benchmarks, i.e., YouTube-VOS (84.1\% J\&F), DAVIS 2017 (84.9\%), and DAVIS 2016 (91.1\%), while keeping more than 3X faster multi-object run-time. Meanwhile, our AOT-T can maintain real-time multi-object speed on the above benchmarks. Based on AOT, we ranked 1st in the 3rd Large-scale VOS Challenge.},
	urldate = {2022-07-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yang, Zongxin and Wei, Yunchao and Yang, Yi},
	year = {2021},
	pages = {2491--2502},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\46KFKD8S\\Yang et al. - 2021 - Associating Objects with Transformers for Video Ob.pdf:application/pdf},
}

@inproceedings{oh_video_2019,
	title = {Video {Object} {Segmentation} {Using} {Space}-{Time} {Memory} {Networks}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Oh_Video_Object_Segmentation_Using_Space-Time_Memory_Networks_ICCV_2019_paper.html},
	urldate = {2022-07-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Oh, Seoung Wug and Lee, Joon-Young and Xu, Ning and Kim, Seon Joo},
	year = {2019},
	pages = {9226--9235},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\WBYRTU5C\\Oh et al. - 2019 - Video Object Segmentation Using Space-Time Memory .pdf:application/pdf},
}

@inproceedings{heller_vitrivr_2022,
	address = {New York, NY, USA},
	series = {{LSC} '22},
	title = {vitrivr at the {Lifelog} {Search} {Challenge} 2022},
	isbn = {978-1-4503-9239-6},
	url = {https://doi.org/10.1145/3512729.3533003},
	doi = {10.1145/3512729.3533003},
	abstract = {In this paper, we present the iteration of the multimedia retrieval system vitrivr participating at LSC 2022. vitrivr is a general-purpose retrieval system which has previously participated at LSC. We describe the system architecture and functionality, and show initial results based on the test and validation topics.},
	urldate = {2022-07-14},
	booktitle = {Proceedings of the 5th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Heller, Silvan and Rossetto, Luca and Sauter, Loris and Schuldt, Heiko},
	month = jun,
	year = {2022},
	keywords = {lifelogging, lifelog search challenge, content-based retrieval, multimedia retrieval},
	pages = {27--31},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\TIHFL3DL\\Heller et al. - 2022 - vitrivr at the Lifelog Search Challenge 2022.pdf:application/pdf},
}

@inproceedings{cheng_per-pixel_2021,
	title = {Per-{Pixel} {Classification} is {Not} {All} {You} {Need} for {Semantic} {Segmentation}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/950a4152c2b4aa3ad78bdd6b366cc179-Abstract.html},
	abstract = {Modern approaches typically formulate semantic segmentation as a per-pixel classification task, while instance-level segmentation is handled with an alternative mask classification. Our key insight: mask classification is sufficiently general to solve both semantic- and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure. Following this observation, we propose MaskFormer, a simple mask classification model which predicts a set of binary masks, each associated with a single global class label prediction. Overall, the proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results. In particular, we observe that MaskFormer outperforms per-pixel classification baselines when the number of classes is large. Our mask classification-based method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K) and panoptic segmentation (52.7 PQ on COCO) models.},
	urldate = {2022-07-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Cheng, Bowen and Schwing, Alex and Kirillov, Alexander},
	year = {2021},
	pages = {17864--17875},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\94JTZRTX\\Cheng et al. - 2021 - Per-Pixel Classification is Not All You Need for S.pdf:application/pdf},
}

@inproceedings{seo_urvos_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{URVOS}: {Unified} {Referring} {Video} {Object} {Segmentation} {Network} with a {Large}-{Scale} {Benchmark}},
	isbn = {978-3-030-58555-6},
	shorttitle = {{URVOS}},
	doi = {10.1007/978-3-030-58555-6_13},
	abstract = {We propose a unified referring video object segmentation network (URVOS). URVOS takes a video and a referring expression as inputs, and estimates the object masks referred by the given language expression in the whole video frames. Our algorithm addresses the challenging problem by performing language-based object segmentation and mask propagation jointly using a single deep neural network with a proper combination of two attention models. In addition, we construct the first large-scale referring video object segmentation dataset called Refer-Youtube-VOS. We evaluate our model on two benchmark datasets including ours and demonstrate the effectiveness of the proposed approach. The dataset is released at https://github.com/skynbe/Refer-Youtube-VOS.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Seo, Seonguk and Lee, Joon-Young and Han, Bohyung},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Referring object segmentation, Video object segmentation},
	pages = {208--223},
}

@inproceedings{margffoy-tuay_dynamic_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Dynamic {Multimodal} {Instance} {Segmentation} {Guided} by {Natural} {Language} {Queries}},
	isbn = {978-3-030-01252-6},
	doi = {10.1007/978-3-030-01252-6_39},
	abstract = {We address the problem of segmenting an object given a natural language expression that describes it. Current techniques tackle this task by either (i) directly or recursively merging linguistic and visual information in the channel dimension and then performing convolutions; or by (ii) mapping the expression to a space in which it can be thought of as a filter, whose response is directly related to the presence of the object at a given spatial coordinate in the image, so that a convolution can be applied to look for the object. We propose a novel method that integrates these two insights in order to fully exploit the recursive nature of language. Additionally, during the upsampling process, we take advantage of the intermediate information generated when downsampling the image, so that detailed segmentations can be obtained. We compare our method against the state-of-the-art approaches in four standard datasets, in which it surpasses all previous methods in six of eight of the splits for this task.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Margffoy-Tuay, Edgar and Pérez, Juan C. and Botero, Emilio and Arbeláez, Pablo},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Dynamic convolutional filters, Instance segmentation, Multimodal interaction, Natural language processing, Referring expressions},
	pages = {656--672},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\4ZAGAIW2\\Margffoy-Tuay et al. - 2018 - Dynamic Multimodal Instance Segmentation Guided by.pdf:application/pdf},
}

@inproceedings{liu_recurrent_2017,
	title = {Recurrent {Multimodal} {Interaction} for {Referring} {Image} {Segmentation}},
	doi = {10.1109/ICCV.2017.143},
	abstract = {In this paper we are interested in the problem of image segmentation given natural language descriptions, i.e. referring expressions. Existing works tackle this problem by first modeling images and sentences independently and then segment images by combining these two types of representations. We argue that learning word-to-image interaction is more native in the sense of jointly modeling two modalities for the image segmentation task, and we propose convolutional multimodal LSTM to encode the sequential interactions between individual words, visual information, and spatial information. We show that our proposed model outperforms the baseline model on benchmark datasets. In addition, we analyze the intermediate output of the proposed multimodal LSTM approach and empirically explain how this approach enforces a more effective word-to-image interaction.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Liu, Chenxi and Lin, Zhe and Shen, Xiaohui and Yang, Jimei and Lu, Xin and Yuille, Alan},
	month = oct,
	year = {2017},
	note = {ISSN: 2380-7504},
	keywords = {Convolution, Feature extraction, Image segmentation, Natural languages, Semantics, Visualization},
	pages = {1280--1289},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\JQCR37HV\\Liu et al. - 2017 - Recurrent Multimodal Interaction for Referring Ima.pdf:application/pdf},
}

@inproceedings{li_referring_2018,
	title = {Referring {Image} {Segmentation} via {Recurrent} {Refinement} {Networks}},
	doi = {10.1109/CVPR.2018.00602},
	abstract = {We address the problem of image segmentation from natural language descriptions. Existing deep learning-based methods encode image representations based on the output of the last convolutional layer. One general issue is that the resulting image representation lacks multi-scale semantics, which are key components in advanced segmentation systems. In this paper, we utilize the feature pyramids inherently existing in convolutional neural networks to capture the semantics at different scales. To produce suitable information flow through the path of feature hierarchy, we propose Recurrent Refinement Network (RRN) that takes pyramidal features as input to refine the segmentation mask progressively. Experimental results on four available datasets show that our approach outperforms multiple baselines and state-of-the-art1.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Li, Ruiyu and Li, Kaican and Kuo, Yi-Chun and Shu, Michelle and Qi, Xiaojuan and Shen, Xiaoyong and Jia, Jiaya},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {Feature extraction, Image segmentation, Natural languages, Semantics, Logic gates, Task analysis, Training},
	pages = {5745--5753},
}

@inproceedings{voigtlaender_online_2017,
	address = {London, UK},
	title = {Online {Adaptation} of {Convolutional} {Neural} {Networks} for {Video} {Object} {Segmentation}},
	isbn = {978-1-901725-60-5},
	url = {http://www.bmva.org/bmvc/2017/papers/paper116/index.html},
	doi = {10.5244/C.31.116},
	language = {en},
	urldate = {2022-07-23},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2017},
	publisher = {British Machine Vision Association},
	author = {Voigtlaender, Paul and Leibe, Bastian},
	year = {2017},
	pages = {116},
	file = {Full Text:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\9G83L7FK\\Voigtlaender and Leibe - 2017 - Online Adaptation of Convolutional Neural Networks.pdf:application/pdf},
}

@inproceedings{caelles_one-shot_2017,
	address = {Honolulu, HI},
	title = {One-{Shot} {Video} {Object} {Segmentation}},
	isbn = {978-1-5386-0457-1},
	url = {https://ieeexplore.ieee.org/document/8100048/},
	doi = {10.1109/CVPR.2017.565},
	language = {en},
	urldate = {2022-07-23},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Caelles, S. and Maninis, K.-K. and Pont-Tuset, J. and Leal-Taixe, L. and Cremers, D. and Van Gool, L.},
	month = jul,
	year = {2017},
	pages = {5320--5329},
	file = {Caelles et al. - 2017 - One-Shot Video Object Segmentation.pdf:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\YMK4K8T3\\Caelles et al. - 2017 - One-Shot Video Object Segmentation.pdf:application/pdf},
}

@article{maninis_video_2019,
	title = {Video {Object} {Segmentation} without {Temporal} {Information}},
	volume = {41},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2018.2838670},
	abstract = {Video Object Segmentation, and video processing in general, has been historically dominated by methods that rely on the temporal consistency and redundancy in consecutive video frames. When the temporal smoothness is suddenly broken, such as when an object is occluded, or some frames are missing in a sequence, the result of these methods can deteriorate significantly. This paper explores the orthogonal approach of processing each frame independently, i.e., disregarding the temporal information. In particular, it tackles the task of semi-supervised video object segmentation: the separation of an object from the background in a video, given its mask in the first frame. We present Semantic One-Shot Video Object Segmentation (OSVOSS), based on a fully-convolutional neural network architecture that is able to successively transfer generic semantic information, learned on ImageNet, to the task of foreground segmentation, and finally to learning the appearance of a single annotated object of the test sequence (hence one shot). We show that instance-level semantic information, when combined effectively, can dramatically improve the results of our previous method, OSVOS. We perform experiments on two recent single-object video segmentation databases, which show that OSVOSS is both the fastest and most accurate method in the state of the art. Experiments on multi-object video segmentation show that OSVOSS obtains competitive results.},
	number = {6},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Maninis, K.-K. and Caelles, S. and Chen, Y. and Pont-Tuset, J. and Leal-Taixé, L. and Cremers, D. and Van Gool, L.},
	month = jun,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Video object segmentation, Image segmentation, Semantics, Task analysis, Training, Computer architecture, convolutional neural networks, instance segmentation, Motion segmentation, Object segmentation, semantic segmentation},
	pages = {1515--1530},
	file = {Accepted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\5VXZZQS5\\Maninis et al. - 2019 - Video Object Segmentation without Temporal Informa.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\R3KW86UY\\8362936.html:text/html},
}

@inproceedings{shin_photocube_2021,
	address = {New York, NY, USA},
	series = {{LSC} '21},
	title = {{PhotoCube} at the {Lifelog} {Search} {Challenge} 2021},
	isbn = {978-1-4503-8533-6},
	url = {https://doi.org/10.1145/3463948.3469073},
	doi = {10.1145/3463948.3469073},
	abstract = {The Lifelog Search Challenge (LSC) is a venue where retrieval system researchers compete in solving tasks to retrieve the correct image from a lifelog collection. At LSC 2021, we introduce the PhotoCube system as a new competitor. PhotoCube is an interactive media retrieval system that considers media items to exist in a hypercube in multidimensional metadata space. To solve tasks, users explore the contents of the hypercube by dynamically (a) applying a variety of filters and (b) projecting the hypercube to a three-dimensional cube that is visualised on screen.},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 4th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Shin, Jihye and Waldau, Alexandra and Duane, Aaron and Jónsson, Björn Þór},
	month = aug,
	year = {2021},
	keywords = {lifelogging, interactive multimedia retrieval, M3},
	pages = {59--63},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	doi = {10.1109/CVPR.2016.90},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2016, {Las} {Vegas}, {NV}, {USA}, {June} 27-30, 2016},
	publisher = {IEEE Computer Society},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\BBEEKSHW\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@article{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	volume = {abs/1907.11692},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	urldate = {2022-05-04},
	journal = {CoRR},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	year = {2019},
	note = {arXiv: 1907.11692},
}

@inproceedings{trang-trung_flexible_2021,
	address = {New York, NY, USA},
	series = {{LSC} '21},
	title = {Flexible {Interactive} {Retrieval} {SysTem} 2.0 for {Visual} {Lifelog} {Exploration} at {LSC} 2021},
	isbn = {978-1-4503-8533-6},
	url = {https://doi.org/10.1145/3463948.3469072},
	doi = {10.1145/3463948.3469072},
	abstract = {With a huge collection of photos and video clips, it is essential to provide an efficient and easy-to-use system for users to retrieve moments of interest with a wide variation of query types. This motivates us to develop and upgrade our flexible interactive retrieval system for visual lifelog exploration. In this paper, we briefly introduce version 2 of our system with the following main features. Our system supports multiple modalities for interaction and query processing, including visual query by meta-data, text query and visual information matching based on a joint embedding model, scene clustering based on visual and location information, flexible temporal event navigation, and query expansion with visual examples. With the flexibility in system architecture, we expect our system can easily integrate new modules to enhance its functionalities.},
	booktitle = {Proceedings of the 4th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Trang-Trung, Hoang-Phuc and Le, Thanh-Cong and Tran, Mai-Khiem and Ninh, Van-Tu and Le, Tu-Khiem and Gurrin, Cathal and Tran, Minh-Triet},
	year = {2021},
	note = {event-place: Taipei, Taiwan},
	keywords = {lifelog, information system, interactive retrieval, component integration, joint embedding model},
	pages = {81--87},
}

@incollection{tran_first_2020,
	address = {New York, NY, USA},
	title = {{FIRST} - {Flexible} {Interactive} {Retrieval} {SysTem} for {Visual} {Lifelog} {Exploration} at {LSC} 2020},
	isbn = {978-1-4503-7136-0},
	url = {https://doi.org/10.1145/3379172.3391726},
	abstract = {Lifelog can provide useful insights of our daily activities. It is essential to provide a flexible way for users to retrieve certain events or moments of interest, corresponding to a wide variation of query types. This motivates us to develop FIRST, a Flexible Interactive Retrieval SysTem, to help users to combine or integrate various query components in a flexible manner to handle different query scenarios, such as visual clustering data based on color histogram, visual similarity, GPS location, or scene attributes. We also employ personalized concept detection and image captioning to enhance image understanding from visual lifelog data, and develop an autoencoder-like approach for query text and image feature mapping. Furthermore, we refine the user interface of the retrieval system to better assist users in query expansion and verifying sequential events in a flexible temporal resolution to control the navigation speed through sequences of images.},
	booktitle = {Proceedings of the {Third} {Annual} {Workshop} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Tran, Minh-Triet and Nguyen, Thanh-An and Tran, Quoc-Cuong and Tran, Mai-Khiem and Nguyen, Khanh and Ninh, Van-Tu and Le, Tu-Khiem and Trang-Trung, Hoang-Phuc and Le, Hoang-Anh and Nguyen, Hai-Dang and Do, Trong-Le and Vo-Ho, Viet-Khoa and Gurrin, Cathal},
	year = {2020},
	pages = {67--72},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	urldate = {2022-05-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2017, {December} 4-9, 2017, {Long} {Beach}, {CA}, {USA}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	pages = {5998--6008},
	file = {Full Text:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\8B5DLMSP\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@inproceedings{trang-trung_lifelog_2020,
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Lifelog {Moment} {Retrieval} with {Self}-{Attention} based {Joint} {Embedding} {Model}},
	volume = {2696},
	url = {http://ceur-ws.org/Vol-2696/paper\_60.pdf},
	booktitle = {Working {Notes} of {CLEF} 2020 - {Conference} and {Labs} of the {Evaluation} {Forum}, {Thessaloniki}, {Greece}, {September} 22-25, 2020},
	publisher = {CEUR-WS.org},
	author = {Trang-Trung, Hoang-Phuc and Le, Hoang-Anh and Tran, Minh-Triet},
	editor = {Cappellato, Linda and Eickhoff, Carsten and Ferro, Nicola and Névéol, Aurélie},
	year = {2020},
}

@inproceedings{duane_virma_2021,
	address = {New York, NY, USA},
	series = {{LSC} '21},
	title = {{ViRMA}: {Virtual} {Reality} {Multimedia} {Analytics} at {LSC} 2021},
	isbn = {978-1-4503-8533-6},
	shorttitle = {{ViRMA}},
	url = {https://doi.org/10.1145/3463948.3469067},
	doi = {10.1145/3463948.3469067},
	abstract = {In this paper we describe the first iteration of the ViRMA prototype system, a novel approach to multimedia analysis in virtual reality and inspired by the M3 data model. We intend to evaluate our approach via the Lifelog Search Challenge (LSC) to serve as a benchmark against other multimedia analytics systems.},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 4th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Duane, Aaron and Jónsson, Bjorn Þór},
	month = aug,
	year = {2021},
	keywords = {lifelogging, virtual reality, human-computer interaction, multimedia analytics},
	pages = {29--34},
}

@inproceedings{spiess_multimodal_2022,
	address = {New York, NY, USA},
	series = {{LSC} '22},
	title = {Multimodal {Interactive} {Lifelog} {Retrieval} with vitrivr-{VR}},
	isbn = {978-1-4503-9239-6},
	url = {https://doi.org/10.1145/3512729.3533008},
	doi = {10.1145/3512729.3533008},
	abstract = {The multimodal nature of lifelog data poses unique challenges for analysis, indexing and interactive retrieval. To address these challenges, the Lifelog Search Challenge (LSC) is an annual evaluation campaign allowing interactive retrieval systems to explore new ideas and measure their performance against each other. This paper describes the virtual reality (VR) multimedia retrieval system vitrivr-VR, with a focus on aspects relevant to the LSC'22, especially the user interaction in VR, the formulation of typical LSC queries, and different options to explore the retrieval results in VR.},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 5th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Spiess, Florian and Schuldt, Heiko},
	month = jun,
	year = {2022},
	keywords = {interactive lifelog retrieval, lifelog search challenge, virtual reality},
	pages = {38--42},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\UPHMGAEA\\Spiess and Schuldt - 2022 - Multimodal Interactive Lifelog Retrieval with vitr.pdf:application/pdf},
}

@inproceedings{ang_lifeconcept_2021,
	address = {New York, NY, USA},
	series = {{LSC} '21},
	title = {{LifeConcept}: {An} {Interactive} {Approach} for {Multimodal} {Lifelog} {Retrieval} through {Concept} {Recommendation}},
	isbn = {978-1-4503-8533-6},
	shorttitle = {{LifeConcept}},
	url = {https://doi.org/10.1145/3463948.3469070},
	doi = {10.1145/3463948.3469070},
	abstract = {The major challenge in visual lifelog retrieval is the semantic gap between textual queries and visual concepts. This paper presents our work on the Lifelog Search Challenge 2021 (LSC'21), an annual comparative benchmarking activity for comparing approaches to interactive retrieval from multimodal lifelogs. We propose LifeConcept, an interactive lifelog search system that is aimed at accelerating the retrieval process and retrieving more precise results. In this work, we introduce several new features such as the number of people, location cluster, and object with color. Moreover, we obtain visual concepts from the images with computer vision models and propose a concept recommendation method to reduce the semantic gap. In this way, users can efficiently set up the related conditions for their requirements and search the desired images with appropriate query terms based on the suggestion.},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 4th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Ang, Wei-Hong and Yen, An-Zi and Chu, Tai-Te and Huang, Hen-Hsen and Chen, Hsin-Hsi},
	month = aug,
	year = {2021},
	keywords = {image retrieval, interactive system, multimodal retrieval, visual lifelogging},
	pages = {47--51},
}

@inproceedings{lokoc_enhanced_2021,
	address = {New York, NY, USA},
	series = {{LSC} '21},
	title = {Enhanced {SOMHunter} for {Known}-item {Search} in {Lifelog} {Data}},
	isbn = {978-1-4503-8533-6},
	url = {https://doi.org/10.1145/3463948.3469074},
	doi = {10.1145/3463948.3469074},
	abstract = {SOMHunter represents a modern light-weight framework for known-item search in datasets of visual data like images or videos. The framework combines an effective W2VV++ text-to-image search approach, a traditional Bayesian like model for maintenance of relevance scores influenced by positive examples, and several types of exploration and exploitation displays. With this initial setting in 2020, already the first prototype of the system turned out to be highly competitive in comparison with other state-of-the-art systems at Video Browser Showdown and Lifelog Search Challenge competitions. In this paper, we present a new version of the system further extending the list of visual data search capabilities. The new version combines localized text queries with collage queries tested at VBS 2021 in two separate systems by our team. Furthermore, the new version of SOMHunter will integrate also the new CLIP text search model recently released by OpenAI. We believe that all the extensions will improve chances to effectively initialize the search that can continue with already supported browsing capabilities.},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 4th {Annual} on {Lifelog} {Search} {Challenge}},
	publisher = {Association for Computing Machinery},
	author = {Lokoč, Jakub and Mejzlik, František and Veselý, Patrik and Souček, Tomáš},
	month = aug,
	year = {2021},
	keywords = {deep learning, self-organizing maps, user feedback, video retrieval},
	pages = {71--73},
}

@inproceedings{li_w2vv_2019,
	address = {New York, NY, USA},
	series = {{MM} '19},
	title = {{W2VV}++: {Fully} {Deep} {Learning} for {Ad}-hoc {Video} {Search}},
	isbn = {978-1-4503-6889-6},
	shorttitle = {{W2VV}++},
	url = {https://doi.org/10.1145/3343031.3350906},
	doi = {10.1145/3343031.3350906},
	abstract = {Ad-hoc video search (AVS) is an important yet challenging problem in multimedia retrieval. Different from previous concept-based methods, we propose a fully deep learning method for query representation learning. The proposed method requires no explicit concept modeling, matching and selection. The backbone of our method is the proposed W2VV++ model, a super version of Word2VisualVec (W2VV) previously developed for visual-to-text matching. W2VV++ is obtained by tweaking W2VV with a better sentence encoding strategy and an improved triplet ranking loss. With these simple yet important changes, W2VV++ brings in a substantial improvement. As our participation in the TRECVID 2018 AVS task and retrospective experiments on the TRECVID 2016 and 2017 data show, our best single model, with an overall inferred average precision (infAP) of 0.157, outperforms the state-of-the-art. The performance can be further boosted by model ensemble using late average fusion, reaching a higher infAP of 0.163. With W2VV++, we establish a new baseline for ad-hoc video search.},
	urldate = {2022-07-18},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Li, Xirong and Xu, Chaoxi and Yang, Gang and Chen, Zhineng and Dong, Jianfeng},
	month = oct,
	year = {2019},
	keywords = {deep learning, ad-hoc video search, cross-modal matching, query representation learning, trecvid benchmarks},
	pages = {1786--1794},
}

@inproceedings{ding_vision-language_2021,
	title = {Vision-{Language} {Transformer} and {Query} {Generation} for {Referring} {Segmentation}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Ding_Vision-Language_Transformer_and_Query_Generation_for_Referring_Segmentation_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-08-02},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Ding, Henghui and Liu, Chang and Wang, Suchen and Jiang, Xudong},
	year = {2021},
	pages = {16321--16330},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\MEMI4QTQ\\Ding et al. - 2021 - Vision-Language Transformer and Query Generation f.pdf:application/pdf;Snapshot:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\F4WRF64L\\Ding_Vision-Language_Transformer_and_Query_Generation_for_Referring_Segmentation_ICCV_2021_pape.html:text/html},
}

@inproceedings{nguyen_visual-language_2022,
	title = {Visual-{Language} {Transformer} for {Referring} {Video} {Object} {Segmentation}},
	abstract = {Referring Video Object Segmentation task (R-VOS) aims to segment the target object in all video frames referred with a language expression. In this work, we present a Visual-Language Transformer (VLFormer), a query-based network to tackle the R-VOS task. Its key component is the visual-language transformer block (VLB), which associates visual and linguistic features with the object queries effectively and simultaneously. We use the object queries as a set of candidate objects, the Transformer decoder with VLB blocks that guide and interact with candidate objects to find the referred target object. The object tracking is achieved automatically by linking the corresponding queries across frames. Afterwards, a post-processing technique is used to refine and re-track the mask prediction among all the frames. Our model ranks 6th place on Track 3: Referring Video Object Segmentation of the CVPR 2022 Youtube-VOS Challenge.},
	language = {en},
	booktitle = {The {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}) {Workshops}, {YouTube}-{VOS}},
	author = {Nguyen, E-Ro and Hoang-Xuan, Nhat and Tran, Minh-Triet},
	year = {2022},
	file = {Nguyen et al. - Visual-Language Transformer for Referring Video Ob.pdf:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\IFCMRPUV\\Nguyen et al. - Visual-Language Transformer for Referring Video Ob.pdf:application/pdf},
}

@inproceedings{tran_v-first_2022,
	address = {Berlin, Heidelberg},
	title = {V-{FIRST}: {A} {Flexible} {Interactive} {Retrieval} {System} for {Video} at {VBS} 2022},
	isbn = {978-3-030-98354-3},
	shorttitle = {V-{FIRST}},
	url = {https://doi.org/10.1007/978-3-030-98355-0_55},
	doi = {10.1007/978-3-030-98355-0_55},
	abstract = {Video retrieval systems have a wide range of applications across multiple domains, therefore the development of user-friendly and efficient systems is necessary. For VBS 2022, we develop a flexible interactive system for video retrieval, namely V-FIRST, that supports two scenarios of usage: query with text descriptions and query with visual examples. We take advantage of both visual and temporal information from videos to extract concepts related to entities, events, scenes, activities, and motion trajectories for video indexing. Our system supports queries with keywords and sentence descriptions as V-FIRST can evaluate the semantic similarities between visual and textual embedding vectors. V-FIRST also allows users to express queries with visual impressions, such as sketches and 2D spatial maps of dominant colors. We use query expansion, elastic temporal video navigation, and intellisense for hints to further boost the performance of our system.},
	urldate = {2022-08-01},
	booktitle = {{MultiMedia} {Modeling}: 28th {International} {Conference}, {MMM} 2022, {Phu} {Quoc}, {Vietnam}, {June} 6–10, 2022, {Proceedings}, {Part} {II}},
	publisher = {Springer-Verlag},
	author = {Tran, Minh-Triet and Hoang-Xuan, Nhat and Trang-Trung, Hoang-Phuc and Le, Thanh-Cong and Tran, Mai-Khiem and Le, Minh-Quan and Le, Tu-Khiem and Ninh, Van-Tu and Gurrin, Cathal},
	month = jun,
	year = {2022},
	keywords = {Color histogram matching, Human-object interaction, Image and video captioning, Interactive system, Moving entity trajectory, Sketch retrieval, Video retrieval},
	pages = {562--568},
}

@inproceedings{wang_end--end_2021,
	title = {End-to-{End} {Video} {Instance} {Segmentation} with {Transformers}},
	doi = {10.1109/CVPR46437.2021.00863},
	abstract = {Video instance segmentation (VIS) is the task that requires simultaneously classifying, segmenting and tracking object instances of interest in video. Recent methods typically develop sophisticated pipelines to tackle this task. Here, we propose a new video instance segmentation framework built upon Transformers, termed VisTR, which views the VIS task as a direct end-to-end parallel sequence decoding/prediction problem. Given a video clip consisting of multiple image frames as input, VisTR outputs the sequence of masks for each instance in the video in order directly. At the core is a new, effective instance sequence matching and segmentation strategy, which supervises and segments instances at the sequence level as a whole. VisTR frames the instance segmentation and tracking in the same perspective of similarity learning, thus considerably simplifying the overall pipeline and is significantly different from existing approaches.Without bells and whistles, VisTR achieves the highest speed among all existing VIS models, and achieves the best result among methods using single model on the YouTube-VIS dataset. For the first time, we demonstrate a much simpler and faster video instance segmentation framework built upon Transformers, achieving competitive accuracy. We hope that VisTR can motivate future research for more video understanding tasks.Code is available at: https://git.io/VisTR},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Wang, Yuqing and Xu, Zhaoliang and Wang, Xinlong and Shen, Chunhua and Cheng, Baoshan and Shen, Hao and Xia, Huaxia},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	keywords = {Image segmentation, Computational modeling, Computer vision, Transformers, Pattern recognition, Pipelines, Transformer cores},
	pages = {8737--8746},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\UR7KXPM7\\Wang et al. - 2021 - End-to-End Video Instance Segmentation with Transf.pdf:application/pdf},
}

@inproceedings{carion_end--end_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	isbn = {978-3-030-58452-8},
	doi = {10.1007/978-3-030-58452-8_13},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {213--229},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\VCIDXRLR\\Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf:application/pdf},
}

@inproceedings{portillo-quintero_straightforward_2021,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Straightforward} {Framework} for {Video} {Retrieval} {Using} {CLIP}},
	volume = {12725},
	url = {https://doi.org/10.1007/978-3-030-77004-4\_1},
	doi = {10.1007/978-3-030-77004-4_1},
	urldate = {2022-08-02},
	booktitle = {Pattern {Recognition} - 13th {Mexican} {Conference}, {MCPR} 2021, {Mexico} {City}, {Mexico}, {June} 23-26, 2021, {Proceedings}},
	publisher = {Springer},
	author = {Portillo-Quintero, Jesús Andrés and Ortiz-Bayliss, José Carlos and Terashima-Marín, Hugo},
	editor = {Roman-Rangel, Edgar and Morales, Ángel Fernando Kuri and Trinidad, José Francisco Martínez and Carrasco-Ochoa, Jesús Ariel and Olvera-López, José Arturo},
	year = {2021},
	pages = {3--12},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\5SMAH6Q6\\Portillo-Quintero et al. - 2021 - A Straightforward Framework for Video Retrieval Us.pdf:application/pdf},
}

@article{hoang-xuan_flexible_nodate,
	title = {Flexible {Interactive} {Retrieval} {SysTem} 2.0 for {Visual} {Lifelog} {Exploration} at {LSC} 2021},
	issn = {1573-7721},
	journal = {Multimedia Tools and Applications},
	author = {Hoang-Xuan, Nhat and Trang-Trung, Hoang-Phuc and Le, Thanh-Cong and Tran, Mai-Khiem and Ninh, Van-Tu and Le, Tu-Khiem and Gurrin, Cathal and Tran, Minh-Triet},
	note = {Submitted for review},
}

@inproceedings{meinhardt_trackformer_2022,
	title = {{TrackFormer}: {Multi}-{Object} {Tracking} {With} {Transformers}},
	shorttitle = {{TrackFormer}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Meinhardt_TrackFormer_Multi-Object_Tracking_With_Transformers_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Meinhardt, Tim and Kirillov, Alexander and Leal-Taixé, Laura and Feichtenhofer, Christoph},
	year = {2022},
	pages = {8844--8854},
	file = {Snapshot:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\SJ9XCD5Z\\Meinhardt_TrackFormer_Multi-Object_Tracking_With_Transformers_CVPR_2022_paper.html:text/html},
}

@inproceedings{loshchilov_decoupled_2022,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {https://openreview.net/forum?id=Bkg6RiCqY7},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case...},
	language = {en},
	urldate = {2022-08-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = feb,
	year = {2022},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\JRJFJUYW\\Loshchilov and Hutter - 2022 - Decoupled Weight Decay Regularization.pdf:application/pdf;Snapshot:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\UKXSHMQZ\\forum.html:text/html},
}

@inproceedings{hu_segmentation_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Segmentation from {Natural} {Language} {Expressions}},
	isbn = {978-3-319-46448-0},
	doi = {10.1007/978-3-319-46448-0_7},
	abstract = {In this paper we approach the novel problem of segmenting an image based on a natural language expression. This is different from traditional semantic segmentation over a predefined set of semantic classes, as e.g., the phrase “two men sitting on the right bench” requires segmenting only the two people on the right bench and no one standing or sitting on another bench. Previous approaches suitable for this task were limited to a fixed set of categories and/or rectangular regions. To produce pixelwise segmentation for the language expression, we propose an end-to-end trainable recurrent and convolutional network model that jointly learns to process visual and linguistic information. In our model, a recurrent neural network is used to encode the referential expression into a vector representation, and a fully convolutional network is used to a extract a spatial feature map from the image and output a spatial response map for the target object. We demonstrate on a benchmark dataset that our model can produce quality segmentation output from the natural language expression, and outperforms baseline methods by a large margin.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Hu, Ronghang and Rohrbach, Marcus and Darrell, Trevor},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Fully convolutional network, Natural language, Recurrent neural network, Segmentation},
	pages = {108--124},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\62ZQ5UCQ\\Hu et al. - 2016 - Segmentation from Natural Language Expressions.pdf:application/pdf},
}

@inproceedings{chen_see-through-text_2019,
	title = {See-{Through}-{Text} {Grouping} for {Referring} {Image} {Segmentation}},
	doi = {10.1109/ICCV.2019.00755},
	abstract = {Motivated by the conventional grouping techniques to image segmentation, we develop their DNN counterpart to tackle the referring variant. The proposed method is driven by a convolutional-recurrent neural network (ConvRNN) that iteratively carries out top-down processing of bottom-up segmentation cues. Given a natural language referring expression, our method learns to predict its relevance to each pixel and derives a See-through-Text Embedding Pixelwise (STEP) heatmap, which reveals segmentation cues of pixel level via the learned visual-textual co-embedding. The ConvRNN performs a top-down approximation by converting the STEP heatmap into a refined one, whereas the improvement is expected from training the network with a classification loss from the ground truth. With the refined heatmap, we update the textual representation of the referring expression by re-evaluating its attention distribution and then compute a new STEP heatmap as the next input to the ConvRNN. Boosting by such collaborative learning, the framework can progressively and simultaneously yield the desired referring segmentation and reasonable attention distribution over the referring sentence. Our method is general and does not rely on, say, the outcomes of object detection from other DNN models, while achieving state-of-the-art performance in all of the four datasets in the experiments.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Chen, Ding-Jie and Jia, Songhao and Lo, Yi-Chen and Chen, Hwann-Tzong and Liu, Tyng-Luh},
	month = oct,
	year = {2019},
	note = {ISSN: 2380-7504},
	keywords = {Convolution, Feature extraction, Image segmentation, Natural languages, Visualization, Task analysis, Heating systems},
	pages = {7453--7462},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\5MPLWWXB\\9009843.html:text/html},
}

@inproceedings{huang_referring_2020,
	title = {Referring {Image} {Segmentation} via {Cross}-{Modal} {Progressive} {Comprehension}},
	doi = {10.1109/CVPR42600.2020.01050},
	abstract = {Referring image segmentation aims at segmenting the foreground masks of the entities that can well match the description given in the natural language expression. Previous approaches tackle this problem using implicit feature interaction and fusion between visual and linguistic modalities, but usually fail to explore informative words of the expression to well align features from the two modalities for accurately identifying the referred entity. In this paper, we propose a Cross-Modal Progressive Comprehension (CMPC) module and a Text-Guided Feature Exchange (TGFE) module to effectively address the challenging task. Concretely, the CMPC module first employs entity and attribute words to perceive all the related entities that might be considered by the expression. Then, the relational words are adopted to highlight the correct entity as well as suppress other irrelevant ones by multimodal graph reasoning. In addition to the CMPC module, we further leverage a simple yet effective TGFE module to integrate the reasoned multimodal features from different levels with the guidance of textual information. In this way, features from multi-levels could communicate with each other and be refined based on the textual context. We conduct extensive experiments on four popular referring segmentation benchmarks and achieve new state-of-the-art performances. Code is available at https://github.com/spyflying/CMPC-Refseg.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Huang, Shaofei and Hui, Tianrui and Liu, Si and Li, Guanbin and Wei, Yunchao and Han, Jizhong and Liu, Luoqi and Li, Bo},
	month = jun,
	year = {2020},
	note = {ISSN: 2575-7075},
	keywords = {Convolution, Feature extraction, Image segmentation, Semantics, Visualization, Cognition, Linguistics},
	pages = {10485--10494},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\T46I766H\\Huang et al. - 2020 - Referring Image Segmentation via Cross-Modal Progr.pdf:application/pdf},
}

@misc{chen_referring_2019,
	title = {Referring {Expression} {Object} {Segmentation} with {Caption}-{Aware} {Consistency}},
	url = {http://arxiv.org/abs/1910.04748},
	doi = {10.48550/arXiv.1910.04748},
	abstract = {Referring expressions are natural language descriptions that identify a particular object within a scene and are widely used in our daily conversations. In this work, we focus on segmenting the object in an image specified by a referring expression. To this end, we propose an end-to-end trainable comprehension network that consists of the language and visual encoders to extract feature representations from both domains. We introduce the spatial-aware dynamic filters to transfer knowledge from text to image, and effectively capture the spatial information of the specified object. To better communicate between the language and visual modules, we employ a caption generation network that takes features shared across both domains as input, and improves both representations via a consistency that enforces the generated sentence to be similar to the given referring expression. We evaluate the proposed framework on two referring expression datasets and show that our method performs favorably against the state-of-the-art algorithms.},
	urldate = {2022-08-15},
	publisher = {arXiv},
	author = {Chen, Yi-Wen and Tsai, Yi-Hsuan and Wang, Tiantian and Lin, Yen-Yu and Yang, Ming-Hsuan},
	month = oct,
	year = {2019},
	note = {arXiv:1910.04748 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted in BMVC'19, project page at https://github.com/wenz116/lang2seg},
	file = {arXiv Fulltext PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\XC2WXWRZ\\Chen et al. - 2019 - Referring Expression Object Segmentation with Capt.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\8RCVEZWE\\1910.html:text/html},
}

@inproceedings{yu_mattnet_2018,
	title = {{MAttNet}: {Modular} {Attention} {Network} for {Referring} {Expression} {Comprehension}},
	shorttitle = {{MAttNet}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_MAttNet_Modular_Attention_CVPR_2018_paper.html},
	urldate = {2022-08-13},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Yu, Licheng and Lin, Zhe and Shen, Xiaohui and Yang, Jimei and Lu, Xin and Bansal, Mohit and Berg, Tamara L.},
	year = {2018},
	pages = {1307--1315},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\PHMYQMJQ\\Yu et al. - 2018 - MAttNet Modular Attention Network for Referring E.pdf:application/pdf;Snapshot:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\DYIBCAMW\\Yu_MAttNet_Modular_Attention_CVPR_2018_paper.html:text/html},
}

@inproceedings{kazemzadeh_referitgame_2014,
	address = {Doha, Qatar},
	title = {{ReferItGame}: {Referring} to {Objects} in {Photographs} of {Natural} {Scenes}},
	shorttitle = {{ReferItGame}},
	url = {https://aclanthology.org/D14-1086},
	doi = {10.3115/v1/D14-1086},
	urldate = {2022-08-15},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
	month = oct,
	year = {2014},
	pages = {787--798},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\S2C4B3XM\\Kazemzadeh et al. - 2014 - ReferItGame Referring to Objects in Photographs o.pdf:application/pdf},
}

@inproceedings{he_mask_2017,
	title = {Mask {R}-{CNN}},
	doi = {10.1109/ICCV.2017.322},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	month = oct,
	year = {2017},
	note = {ISSN: 2380-7504},
	keywords = {Feature extraction, Image segmentation, Semantics, Object detection, Quantization (signal), Robustness},
	pages = {2980--2988},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\M9MDINJY\\footnotes.html:text/html},
}

@inproceedings{jing_locate_2021,
	title = {Locate then {Segment}: {A} {Strong} {Pipeline} for {Referring} {Image} {Segmentation}},
	shorttitle = {Locate then {Segment}},
	doi = {10.1109/CVPR46437.2021.00973},
	abstract = {Referring image segmentation aims to segment the objects referred by a natural language expression. Previous methods usually focus on designing an implicit and recurrent feature interaction mechanism to fuse the visual-linguistic features to directly generate the final segmentation mask without explicitly modeling the localization information of the referent instances. To tackle these problems, we view this task from another perspective by decoupling it into a "Locate-Then-Segment" (LTS) scheme. Given a language expression, people generally first perform attention to the corresponding target image regions, then generate a fine segmentation mask about the object based on its context. The LTS first extracts and fuses both visual and textual features to get a cross-modal representation, then applies a cross-model interaction on the visual-textual features to locate the referred object with position prior, and finally generates the segmentation result with a light-weight segmentation network. Our LTS is simple but surprisingly effective. On three popular benchmark datasets, the LTS outperforms all the previous state-of-the-arts methods by a large margin (e.g., +3.2\% on RefCOCO+ and +3.4\% on RefCOCOg). In addition, our model is more interpretable with explicitly locating the object, which is also proved by visualization experiments. We believe this framework is promising to serve as a strong baseline for referring image segmentation.},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Jing, Ya and Kong, Tao and Wang, Wei and Wang, Liang and Li, Lei and Tan, Tieniu},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	keywords = {Feature extraction, Image segmentation, Visualization, Object segmentation, Pipelines, Fuses, Location awareness},
	pages = {9853--9862},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\MV4YCFHQ\\Jing et al. - 2021 - Locate then Segment A Strong Pipeline for Referri.pdf:application/pdf},
}

@inproceedings{hui_linguistic_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Linguistic {Structure} {Guided} {Context} {Modeling} for {Referring} {Image} {Segmentation}},
	isbn = {978-3-030-58607-2},
	doi = {10.1007/978-3-030-58607-2_4},
	abstract = {Referring image segmentation aims to predict the foreground mask of the object referred by a natural language sentence. Multimodal context of the sentence is crucial to distinguish the referent from the background. Existing methods either insufficiently or redundantly model the multimodal context. To tackle this problem, we propose a “gather-propagate-distribute” scheme to model multimodal context by cross-modal interaction and implement this scheme as a novel Linguistic Structure guided Context Modeling (LSCM) module. Our LSCM module builds a Dependency Parsing Tree suppressed Word Graph (DPT-WG) which guides all the words to include valid multimodal context of the sentence while excluding disturbing ones through three steps over the multimodal feature, i.e., gathering, constrained propagation and distributing. Extensive experiments on four benchmarks demonstrate that our method outperforms all the previous state-of-the-arts.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Hui, Tianrui and Liu, Si and Huang, Shaofei and Li, Guanbin and Yu, Sansi and Zhang, Faxi and Han, Jizhong},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Dependency Parsing Tree, Graph propagation, Linguistic structure, Multimodal context, Referring segmentation},
	pages = {59--75},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\EJWDH83H\\Hui et al. - 2020 - Linguistic Structure Guided Context Modeling for R.pdf:application/pdf},
}

@inproceedings{radford_learning_2021-1,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {https://proceedings.mlr.press/v139/radford21a.html},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
	language = {en},
	urldate = {2022-08-13},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8748--8763},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\WTPC97NV\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf;Supplementary PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\EYP2RFWT\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf},
}

@inproceedings{liu_learning_2019,
	title = {Learning to {Assemble} {Neural} {Module} {Tree} {Networks} for {Visual} {Grounding}},
	doi = {10.1109/ICCV.2019.00477},
	abstract = {Visual grounding, a task to ground (i.e., localize) natural language in images, essentially requires composite visual reasoning. However, existing methods over-simplify the composite nature of language into a monolithic sentence embedding or a coarse composition of subject-predicate-object triplet. In this paper, we propose to ground natural language in an intuitive, explainable, and composite fashion as it should be. In particular, we develop a novel modular network called Neural Module Tree network (NMTree) that regularizes the visual grounding along the dependency parsing tree of the sentence, where each node is a neural module that calculates visual attention according to its linguistic feature, and the grounding score is accumulated in a bottom-up direction where as needed. NMTree disentangles the visual grounding from the composite reasoning, allowing the former to only focus on primitive and easy-to-generalize patterns. To reduce the impact of parsing errors, we train the modules and their assembly end-to-end by using the Gumbel-Softmax approximation and its straight-through gradient estimator, accounting for the discrete nature of module assembly. Overall, the proposed NMTree consistently outperforms the state-of-the-arts on several benchmarks. Qualitative results show explainable grounding score calculation in great detail.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Liu, Daqing and Zhang, Hanwang and Zha, Zheng-Jun and Wu, Feng},
	month = oct,
	year = {2019},
	note = {ISSN: 2380-7504},
	keywords = {Natural languages, Visualization, Task analysis, Training, Cognition, Artificial neural networks, Grounding},
	pages = {4672--4681},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\92VBCF9Q\\9009000.html:text/html;Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\V3KF77IW\\Liu et al. - 2019 - Learning to Assemble Neural Module Tree Networks f.pdf:application/pdf},
}

@inproceedings{shi_key-word-aware_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Key-{Word}-{Aware} {Network} for {Referring} {Expression} {Image} {Segmentation}},
	isbn = {978-3-030-01231-1},
	doi = {10.1007/978-3-030-01231-1_3},
	abstract = {Referring expression image segmentation aims to segment out the object referred by a natural language query expression. Without considering the specific properties of visual and textual information, existing works usually deal with this task by directly feeding a foreground/background classifier with cascaded image and text features, which are extracted from each image region and the whole query, respectively. On the one hand, they ignore that each word in a query expression makes different contributions to identify the desired object, which requires a differential treatment in extracting text feature. On the other hand, the relationships of different image regions are not considered as well, even though they are greatly important to eliminate the undesired foreground object in accordance with specific query. To address aforementioned issues, in this paper, we propose a key-word-aware network, which contains a query attention model and a key-word-aware visual context model. In extracting text features, the query attention model attends to assign higher weights for the words which are more important for identifying object. Meanwhile, the key-word-aware visual context model describes the relationships among different image regions, according to corresponding query. Our proposed method outperforms state-of-the-art methods on two referring expression image segmentation databases.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Shi, Hengcan and Li, Hongliang and Meng, Fanman and Wu, Qingbo},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {Key word extraction, Key-word-aware visual context, Query attention, Referring expression image segmentation},
	pages = {38--54},
}

@inproceedings{mao_generation_2016,
	title = {Generation and {Comprehension} of {Unambiguous} {Object} {Descriptions}},
	doi = {10.1109/CVPR.2016.9},
	abstract = {We propose a method that can generate an unambiguous description (known as a referring expression) of a specific object or region in an image, and which can also comprehend or interpret such an expression to infer which object is being described. We show that our method outperforms previous methods that generate descriptions of objects without taking into account other potentially ambiguous objects in the scene. Our model is inspired by recent successes of deep learning methods for image captioning, but while image captioning is difficult to evaluate, our task allows for easy objective evaluation. We also present a new large-scale dataset for referring expressions, based on MSCOCO. We have released the dataset and a toolbox for visualization and evaluation, see https://github.com/ mjhucla/Google\_Refexp\_toolbox.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan and Murphy, Kevin},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Visualization, Training, Automobiles, Context, Google, Machine learning, Recurrent neural networks},
	pages = {11--20},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\2A5GAEAQ\\7780378.html:text/html;Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\A7JH857M\\Mao et al. - 2016 - Generation and Comprehension of Unambiguous Object.pdf:application/pdf},
}

@inproceedings{mao_generation_2016-1,
	title = {Generation and {Comprehension} of {Unambiguous} {Object} {Descriptions}},
	doi = {10.1109/CVPR.2016.9},
	abstract = {We propose a method that can generate an unambiguous description (known as a referring expression) of a specific object or region in an image, and which can also comprehend or interpret such an expression to infer which object is being described. We show that our method outperforms previous methods that generate descriptions of objects without taking into account other potentially ambiguous objects in the scene. Our model is inspired by recent successes of deep learning methods for image captioning, but while image captioning is difficult to evaluate, our task allows for easy objective evaluation. We also present a new large-scale dataset for referring expressions, based on MSCOCO. We have released the dataset and a toolbox for visualization and evaluation, see https://github.com/ mjhucla/Google\_Refexp\_toolbox.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan and Murphy, Kevin},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Visualization, Training, Automobiles, Context, Google, Machine learning, Recurrent neural networks},
	pages = {11--20},
	file = {Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\Z5MAFXSG\\Mao et al. - 2016 - Generation and Comprehension of Unambiguous Object.pdf:application/pdf},
}

@inproceedings{feng_encoder_2021,
	title = {Encoder {Fusion} {Network} with {Co}-{Attention} {Embedding} for {Referring} {Image} {Segmentation}},
	doi = {10.1109/CVPR46437.2021.01525},
	abstract = {Recently, referring image segmentation has aroused widespread interest. Previous methods perform the multi-modal fusion between language and vision at the decoding side of the network. And, linguistic feature interacts with visual feature of each scale separately, which ignores the continuous guidance of language to multi-scale visual features. In this work, we propose an encoder fusion network (EFN), which transforms the visual encoder into a multi-modal feature learning network, and uses language to refine the multi-modal features progressively. Moreover, a co-attention mechanism is embedded in the EFN to realize the parallel update of multi-modal features, which can promote the consistent of the cross-modal information representation in the semantic space. Finally, we propose a boundary enhancement module (BEM) to make the network pay more attention to the fine structure. The experiment results on four benchmark datasets demonstrate that the proposed approach achieves the state-of-the-art performance under different evaluation metrics without any post-processing.},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Feng, Guang and Hu, Zhiwei and Zhang, Lihe and Lu, Huchuan},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	keywords = {Image segmentation, Semantics, Visualization, Grounding, Information representation, Measurement, Transforms},
	pages = {15501--15510},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\GXASLALS\\9578505.html:text/html},
}

@inproceedings{fu_dual_2019,
	title = {Dual {Attention} {Network} for {Scene} {Segmentation}},
	isbn = {978-1-72813-293-8},
	url = {https://www.computer.org/csdl/proceedings-article/cvpr/2019/329300d141/1gyrvRG7yPm},
	doi = {10.1109/CVPR.2019.00326},
	abstract = {In this paper, we address the scene segmentation task by capturing rich contextual dependencies based on the self-attention mechanism. Unlike previous works that capture contexts by multi-scale features fusion, we propose a Dual Attention Networks (DANet) to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of traditional dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the features at each position by a weighted sum of the features at all positions. Similar features would be related to each other regardless of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by integrating associated features among all channel maps. We sum the outputs of the two attention modules to further improve feature representation which contributes to more precise segmentation results. We achieve new state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5\% on Cityscapes test set is achieved without using coarse data.},
	language = {English},
	urldate = {2022-08-13},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE Computer Society},
	author = {Fu, Jun and Liu, Jing and Tian, Haijie and Li, Yong and Bao, Yongjun and Fang, Zhiwei and Lu, Hanqing},
	month = jun,
	year = {2019},
	pages = {3141--3149},
	file = {Snapshot:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\TY73SGU9\\1gyrvRG7yPm.html:text/html;Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\BLR6CV8M\\Fu et al. - 2019 - Dual Attention Network for Scene Segmentation.pdf:application/pdf},
}

@inproceedings{he_deep_2016-1,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Image segmentation, Visualization, Training, Complexity theory, Degradation, Image recognition, Neural networks},
	pages = {770--778},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\4EYN3MWZ\\7780459.html:text/html;Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\3V8XUS5N\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@inproceedings{devlin_bert_2019-1,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-08-15},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\RPQ7NT6M\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017-1,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2022-08-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\PH326SK2\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@inproceedings{ye_cross-modal_2019,
	title = {Cross-{Modal} {Self}-{Attention} {Network} for {Referring} {Image} {Segmentation}},
	doi = {10.1109/CVPR.2019.01075},
	abstract = {We consider the problem of referring image segmentation. Given an input image and a natural language expression, the goal is to segment the object referred by the language expression in the image. Existing works in this area treat the language expression and the input image separately in their representations. They do not sufficiently capture long-range correlations between these two modalities. In this paper, we propose a cross-modal self-attention (CMSA) module that effectively captures the long-range dependencies between linguistic and visual features. Our model can adaptively focus on informative words in the referring expression and important regions in the input image. In addition, we propose a gated multi-level fusion module to selectively integrate self-attentive cross-modal features corresponding to different levels in the image. This module controls the information flow of features at different levels. We validate the proposed approach on four evaluation datasets. Our proposed approach consistently outperforms existing state-of-the-art methods.},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Ye, Linwei and Rochan, Mrigank and Liu, Zhi and Wang, Yang},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {Segmentation, Grouping and Shape, Scene Analysis and Understanding, Vision + Language},
	pages = {10494--10503},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\FVEANLRL\\8953566.html:text/html;Submitted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\8STPDJGS\\Ye et al. - 2019 - Cross-Modal Self-Attention Network for Referring I.pdf:application/pdf},
}

@inproceedings{he_adaptive_2019,
	title = {Adaptive {Pyramid} {Context} {Network} for {Semantic} {Segmentation}},
	doi = {10.1109/CVPR.2019.00770},
	abstract = {Recent studies witnessed that context features can significantly improve the performance of deep semantic segmentation networks. Current context based segmentation methods differ with each other in how to construct context features and perform differently in practice. This paper firstly introduces three desirable properties of context features in segmentation task. Specially, we find that Global-guided Local Affinity (GLA) can play a vital role in constructing effective context features, while this property has been largely ignored in previous works. Based on this analysis, this paper proposes Adaptive Pyramid Context Network (APCNet) for semantic segmentation. APCNet adaptively constructs multi-scale contextual representations with multiple well-designed Adaptive Context Modules (ACMs). Specifically, each ACM leverages a global image representation as a guidance to estimate the local affinity coefficients for each sub-region, and then calculates a context vector with these affinities. We empirically evaluate our APCNet on three semantic segmentation and scene parsing datasets, including PASCAL VOC 2012, Pascal-Context, and ADE20K dataset. Experimental results show that APCNet achieves state-of-the-art performance on all three benchmarks, and obtains a new record 84.2\% on PASCAL VOC 2012 test set without MS COCO pre-trained and any post-processing.},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Junjun and Deng, Zhongying and Zhou, Lei and Wang, Yali and Qiao, Yu},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {Image segmentation, Semantics, Computer vision, Pattern recognition, Segmentation, Grouping and Shape, Adaptive systems, Benchmark testing, Deep Learning, Image representation, Vision Applications and Systems},
	pages = {7511--7520},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\N3GGH77A\\8954288.html:text/html},
}

@inproceedings{yang_lavt_2022,
	title = {{LAVT}: {Language}-{Aware} {Vision} {Transformer} for {Referring} {Image} {Segmentation}},
	shorttitle = {{LAVT}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Yang_LAVT_Language-Aware_Vision_Transformer_for_Referring_Image_Segmentation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Yang, Zhao and Wang, Jiaqi and Tang, Yansong and Chen, Kai and Zhao, Hengshuang and Torr, Philip H. S.},
	year = {2022},
	pages = {18155--18165},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\ZXVN9ELN\\Yang et al. - 2022 - LAVT Language-Aware Vision Transformer for Referr.pdf:application/pdf;Snapshot:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\ZEP4F32S\\Yang_LAVT_Language-Aware_Vision_Transformer_for_Referring_Image_Segmentation_CVPR_2022_paper.html:text/html},
}

@inproceedings{kim_restr_2022,
	title = {{ReSTR}: {Convolution}-{Free} {Referring} {Image} {Segmentation} {Using} {Transformers}},
	shorttitle = {{ReSTR}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Kim_ReSTR_Convolution-Free_Referring_Image_Segmentation_Using_Transformers_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Kim, Namyup and Kim, Dongwon and Lan, Cuiling and Zeng, Wenjun and Kwak, Suha},
	year = {2022},
	pages = {18145--18154},
	file = {Full Text PDF:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\H8WBYCJV\\Kim et al. - 2022 - ReSTR Convolution-Free Referring Image Segmentati.pdf:application/pdf;Snapshot:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\KV3INC66\\Kim_ReSTR_Convolution-Free_Referring_Image_Segmentation_Using_Transformers_CVPR_2022_paper.html:text/html},
}

@inproceedings{gurrin_introduction_2021,
	address = {New York, NY, USA},
	series = {{ICMR} '21},
	title = {Introduction to the {Fourth} {Annual} {Lifelog} {Search} {Challenge}, {LSC}'21},
	isbn = {978-1-4503-8463-6},
	url = {https://doi.org/10.1145/3460426.3470945},
	doi = {10.1145/3460426.3470945},
	abstract = {The Lifelog Search Challenge (LSC) is an annual benchmarking challenge for comparing approaches to interactive retrieval from multi-modal lifelogs. LSC'21, the fourth challenge, attracted sixteen participants, each of which had developed interactive retrieval systems for large multimodal lifelogs. These interactive retrieval systems participated in a comparative evaluation in front of an online live-audience at the LSC workshop at ACM ICMR'21. This overview presents the motivation for LSC'21, the lifelog dataset used in the competition, and the participating systems.},
	urldate = {2022-08-22},
	booktitle = {Proceedings of the 2021 {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Gurrin, Cathal and Jónsson, Björn Þór and Schöffmann, Klaus and Dang-Nguyen, Duc-Tien and Lokoč, Jakub and Tran, Minh-Triet and Hürst, Wolfgang and Rossetto, Luca and Healy, Graham},
	month = aug,
	year = {2021},
	keywords = {benchmarking, interactive retrieval systems, lifelog},
	pages = {690--691},
	file = {Accepted Version:C\:\\Users\\NHAT-LAPTOP\\Zotero\\storage\\KK96JSU3\\Gurrin et al. - 2021 - Introduction to the Fourth Annual Lifelog Search C.pdf:application/pdf},
}

@inproceedings{nguyen_vlformer_2022,
	title = {{VLFormer}: {Visual}-{Linguistic} {Transformer} for {Referring} {Image} {Segmentation}},
	abstract = {The referring image segmentation task aims to segment a referred object from an image using a natural language expression. The query expression in referring image segmentation typically describes the relationship between the target object and others. Therefore, several objects may appear in the expression, and the model must carefully understand the language expression and select the correct object that the expression refers to. In this work, we introduce a unified and simple query-based framework named VLFormer. Concretely, we use a small set of object queries to represent candidate objects and design a mechanism to generate the fine-grained object queries by utilizing language and multi-scale vision information. More specifically, we propose a Visual-Linguistic Transformer Block, which produces a richer representation of the objects by associating visual and linguistic features with the object queries effectively and simultaneously.
At the same time, we leverage the ability to extract linguistic features from CLIP, which has a great potential for compatibility with visual information.
Without bells and whistles, our proposed method significantly outperforms the previous state-of-the-art methods by large margins on three referring image segmentation datasets: RefCOCO, RefCOCO+, and G-Ref.},
	language = {en},
	booktitle = {Thirty-{Seventh} {AAAI} {Conference} on {Artificial} {Intelligence} - {AAAI} 2023},
	author = {Nguyen, E-Ro and Hoang-Xuan, Nhat and Nguyen, V. Tam and Tran, Minh-Triet},
	year = {2022},
	note = {Submitted for review},
}

@article{gao_clip2tv_2021,
	title = {{CLIP2TV}: {An} {Empirical} {Study} on {Transformer}-based {Methods} for {Video}-{Text} {Retrieval}},
	volume = {abs/2111.05610},
	shorttitle = {{CLIP2TV}},
	url = {https://arxiv.org/abs/2111.05610},
	urldate = {2022-08-22},
	journal = {CoRR},
	author = {Gao, Zijian and Liu, Jingyu and Chen, Sheng and Chang, Dedan and Zhang, Hao and Yuan, Jinwei},
	year = {2021},
	note = {arXiv: 2111.05610},
}
